{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF with clustering reduction\n",
    "\n",
    "Based on the paper: <i>Fuzzy Bag-of-Words Model for Document Representation</i>. The idea is to use clustering to condense individual words into semantically-similar groupings of words prior to generating TF-IDF features. This, among other things, increases the co-occurrence information for rare terms, which is important when text is exceedingly short. Note: \"Topic\" is used to describe the grouping of words with similar meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter,defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import ast\n",
    "import json\n",
    "import spacy\n",
    "from sklearn.cluster import AgglomerativeClustering,KMeans # for clustering word embeddings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/word_vectors.json\") as word_vector_file: # processed word embeddings\n",
    "    word_embeddings_dict = json.load(word_vector_file)\n",
    "for word in word_embeddings_dict: # changing all word vectors to np.arrays\n",
    "    word_embeddings_dict[word]['vec'] = np.array(word_embeddings_dict[word]['vec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = pd.read_csv(\"../data/cleaned_data.csv\")\n",
    "cdf['problem_tokens'] = cdf['problem_tokens'].apply(lambda list_string: ast.literal_eval(list_string))\n",
    "problem_tokens = cdf['problem_tokens'].tolist()\n",
    "corpus = problem_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7572, 5021)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens = []\n",
    "for problem in problem_tokens:\n",
    "    all_tokens += problem\n",
    "\n",
    "token_counts = Counter(all_tokens)\n",
    "common_words = [] # defined as having more than a single occurrence\n",
    "uncommon_words = []\n",
    "for word in token_counts:\n",
    "    if token_counts[word]>1:\n",
    "        common_words.append(word)\n",
    "    else:\n",
    "        uncommon_words.append(word)\n",
    "        \n",
    "len(common_words),len(uncommon_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33.0, 2.0)\n"
     ]
    }
   ],
   "source": [
    "# summary stats of distribution of token occurrences (in essence the data is largely filled with short texts)\n",
    "sorted_counts = list(token_counts.items())\n",
    "sorted_counts.sort(key=lambda tup: tup[1])\n",
    "counts = [tup[1] for tup in sorted_counts]\n",
    "unigram_avg = round(float(np.average(counts)),1)\n",
    "unigram_median = round(float(np.median(counts)),1)\n",
    "\n",
    "print((unigram_avg,unigram_median))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12593, 300), 12593)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vectors for each individual word\n",
    "all_words = common_words+uncommon_words\n",
    "all_word_vectors = []\n",
    "\n",
    "for word in all_words:\n",
    "    vec = word_embeddings_dict[word]['vec']\n",
    "    all_word_vectors.append(vec)\n",
    "    \n",
    "all_word_vectors = np.stack(all_word_vectors)\n",
    "all_word_vectors.shape,len(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard condensed representations using only common words\n",
    "\n",
    "Hierarchical clustering is utilized due to the efficiency of being able to set a distance threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_assignments_per_word(clusterer,words,word_embeddings):\n",
    "    \"\"\" returns both the words per topic cluster and the topic cluster per word\n",
    "    \"\"\"\n",
    "    labels = clusterer.fit_predict(word_embeddings)\n",
    "    word_cluster_assignment = {} # assigns each word to a unique cluster\n",
    "    cluster_words_assignment = defaultdict(list) # stores words assigned to each cluster\n",
    "    for i,word in enumerate(words):\n",
    "        cluster_label = labels[i]\n",
    "        word_cluster_assignment[word] = cluster_label\n",
    "        cluster_words_assignment[cluster_label].append(word)\n",
    "    \n",
    "    return word_cluster_assignment,cluster_words_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_stats_for_topic_clusters(token_counts,cluster_words_assignment):\n",
    "    \"\"\" returns the median and mean of the number of occurrences for each topic\n",
    "        -the count for a given topic is the summation of the counts for the individual tokens in that topic\n",
    "    \"\"\"\n",
    "    new_topic_counts = []\n",
    "    for topic,words_in_topic in cluster_words_assignment.items():\n",
    "        total_topic_count = 0\n",
    "        for word in words_in_topic:\n",
    "            total_topic_count += token_counts[word]\n",
    "        new_topic_counts.append(total_topic_count)\n",
    "        \n",
    "    mean = round(float(np.average(new_topic_counts)),1)\n",
    "    median = round(float(np.median(new_topic_counts)),1)\n",
    "    return mean,median    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7572, 300), 7572)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_word_vectors = []\n",
    "\n",
    "for word in common_words:\n",
    "    vec = word_embeddings_dict[word]['vec']\n",
    "    common_word_vectors.append(vec)\n",
    "    \n",
    "common_word_vectors = np.stack(common_word_vectors)\n",
    "print(common_word_vectors.shape,len(common_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_features_for_reduced_topics_common(old_corpus,word_cluster_assignment,common_words):\n",
    "    \"\"\" returns the tf-idf features after reducing the keywords into their associated cluster topic assignments\n",
    "        -removes the uncommon words\n",
    "    \"\"\"\n",
    "    common_words = set(common_words)\n",
    "    corpus = [] # removing the uncommon words from the old corpus\n",
    "    for words in old_corpus:\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            if word in common_words:\n",
    "                new_words.append(word)\n",
    "        corpus.append(new_words)\n",
    "    \n",
    "    updated_corpus = [] # generating a new corpus which represents words in a grouping as a single \"word\"\n",
    "    for tokens in corpus:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            token_topic = \"topic\"+str(word_cluster_assignment[token]) # cluster this token belongs to\n",
    "            new_tokens.append(token_topic)\n",
    "        updated_corpus.append(new_tokens)\n",
    "        \n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,1),max_features=None,lowercase=False,preprocessor=lambda x:x,tokenizer=lambda x:x)\n",
    "    tfidf_vectors = vectorizer.fit_transform(updated_corpus)\n",
    "    tfidf_vectors = np.array(tfidf_vectors.todense())\n",
    "    return tfidf_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67152, 4440)\n"
     ]
    }
   ],
   "source": [
    "clusterer = AgglomerativeClustering(n_clusters=None,compute_full_tree=True,distance_threshold=6.0)\n",
    "word_cluster_assignment,cluster_words_assignment = get_topic_assignments_per_word(clusterer,common_words,common_word_vectors)\n",
    "tfidf_vectors = get_tfidf_features_for_reduced_topics_common(corpus,word_cluster_assignment,common_words)\n",
    "print(tfidf_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67152, 3048)\n"
     ]
    }
   ],
   "source": [
    "clusterer = AgglomerativeClustering(n_clusters=None,compute_full_tree=True,distance_threshold=7.0)\n",
    "word_cluster_assignment,cluster_words_assignment = get_topic_assignments_per_word(clusterer,common_words,common_word_vectors)\n",
    "tfidf_vectors = get_tfidf_features_for_reduced_topics_common(corpus,word_cluster_assignment,common_words)\n",
    "print(tfidf_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variant of the standard approach in which only uncommon words are grouped\n",
    "\n",
    "In this case, uncommon words are defined to have <= median=2 occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5952, 6641)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_common_words = [] # <= median occurrences is considered a common word\n",
    "median_uncommon_words = []\n",
    "for word in token_counts:\n",
    "    if token_counts[word]>2:\n",
    "        median_common_words.append(word)\n",
    "    else:\n",
    "        median_uncommon_words.append(word)\n",
    "        \n",
    "len(median_common_words),len(median_uncommon_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6641, 300), 6641)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_uncommon_word_vectors = []\n",
    "\n",
    "for word in median_uncommon_words:\n",
    "    vec = word_embeddings_dict[word]['vec']\n",
    "    median_uncommon_word_vectors.append(vec)\n",
    "    \n",
    "median_uncommon_word_vectors = np.stack(median_uncommon_word_vectors)\n",
    "median_uncommon_word_vectors.shape,len(median_uncommon_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_features_for_reduced_topics_cluster_uncommon_words(corpus,word_cluster_assignment,cluster_words_assignment,median_common_words):\n",
    "    \"\"\" returns the tf-idf features after reducing the keywords into their associated cluster topic assignments\n",
    "        -stacks standard tf-idf vectors made from common words with one made from grouping uncommon words\n",
    "    \"\"\"\n",
    "    median_common_words = set(median_common_words)\n",
    "    corpus1 = [] # updated corpus made up of only common words\n",
    "    for tokens in corpus:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            if token in median_common_words: # this token is a common word\n",
    "                new_tokens.append(token)\n",
    "        corpus1.append(new_tokens)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,1),max_features=None,lowercase=False,preprocessor=lambda x:x,tokenizer=lambda x:x)\n",
    "    tfidf_vectors1 = vectorizer.fit_transform(corpus1)\n",
    "    tfidf_vectors1 = np.array(tfidf_vectors1.todense())\n",
    "        \n",
    "    corpus2 = [] # updated corpus made up of only uncommon words\n",
    "    for tokens in corpus:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            if token not in median_common_words: # only using uncommon words for this set of features\n",
    "                token_topic = \"topic\"+str(word_cluster_assignment[token]) # cluster this token belongs to\n",
    "                new_tokens.append(token_topic)\n",
    "        corpus2.append(new_tokens)\n",
    "        \n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,1),max_features=None,lowercase=False,preprocessor=lambda x:x,tokenizer=lambda x:x)\n",
    "    tfidf_vectors2 = vectorizer.fit_transform(corpus2)\n",
    "    tfidf_vectors2 = np.array(tfidf_vectors2.todense())\n",
    "    \n",
    "    final_tfidf = np.hstack([tfidf_vectors1,tfidf_vectors2])\n",
    "    return final_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67152, 11041)\n"
     ]
    }
   ],
   "source": [
    "clusterer = AgglomerativeClustering(n_clusters=None,compute_full_tree=True,distance_threshold=6.0)\n",
    "word_cluster_assignment,cluster_words_assignment = get_topic_assignments_per_word(clusterer,median_uncommon_words,median_uncommon_word_vectors)\n",
    "tfidf_vectors = get_tfidf_features_for_reduced_topics_cluster_uncommon_words(corpus,word_cluster_assignment,cluster_words_assignment,median_common_words)\n",
    "print(tfidf_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67152, 9709)\n"
     ]
    }
   ],
   "source": [
    "clusterer = AgglomerativeClustering(n_clusters=None,compute_full_tree=True,distance_threshold=7.0)\n",
    "word_cluster_assignment,cluster_words_assignment = get_topic_assignments_per_word(clusterer,median_uncommon_words,median_uncommon_word_vectors)\n",
    "tfidf_vectors = get_tfidf_features_for_reduced_topics_cluster_uncommon_words(corpus,word_cluster_assignment,cluster_words_assignment,median_common_words)\n",
    "print(tfidf_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
