{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative feature expansion\n",
    "\n",
    "Based on the paper: <i>Language Model Information Retrieval with Document Expansion</i>. The idea is to reduce feature sparsity by considering/incorporating neighborhood examples. As put in the paper, \"we are looking for a new enlarged document d\\` for each document d in a text collection, such that the new document d\\` can be used to estimate the hidden generative model of d more accurately\". The document features used are TF-IDF. For comparing the similarity between documents/examples, we use average word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter,defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import heapq\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average embeddings for each of the examples are used as the \"key\"\n",
    "avg_emb = np.load(\"../data/features/avg_embedding.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_norms = {} # caching for easy access later\n",
    "for i in range(0,len(avg_emb)):\n",
    "    vec_norm = np.linalg.norm(avg_emb[i])\n",
    "    vector_norms[i]=vec_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the 100 closest neighbors for each of the datapoints\n",
    "closest_neighbors = {} # for each datapoint, returns info about the 100 closest other datapoints (indices & similarity)\n",
    "for i in range(0,len(avg_emb)):\n",
    "    other_i_s = [num for num in range(0,len(avg_emb)) if num != i] # getting all neighbors\n",
    "    min_heap = [] # keeping the maximum cosine similarities, datapoints are tups:[(cos_sim,other_index),...]\n",
    "    for other_i in other_i_s:\n",
    "        cos_sim = float(np.dot(avg_emb[i],avg_emb[other_i])/(vector_norms[i]*vector_norms[other_i]))\n",
    "        if len(min_heap)<100:\n",
    "            heapq.heappush(min_heap,(cos_sim,other_i))\n",
    "        else:\n",
    "            if cos_sim > min_heap[0][0]:\n",
    "                heapq.heappushpop(min_heap,(cos_sim,other_i))\n",
    "    \n",
    "    sorted_closest_neighbors = heapq.nlargest(100,min_heap)\n",
    "    closest_neighbors[i] = sorted_closest_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"closest_100_neighbors.json\",\"w+\") as out_file:\n",
    "    json.dump(closest_neighbors,out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_neighbors_file = open(\"closest_100_neighbors.json\")\n",
    "closest_neighbors = json.load(closest_neighbors_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature expansion implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_features(feature_fname,closest_neighbors,k=10,lam=0.5):\n",
    "    \"\"\" Updates the features and returns the mean/median increase in number of unique features across datapoints\n",
    "    args:\n",
    "        k: the number of \"similar\" documents to consider (neighborhood size)\n",
    "        lam: the amount of attention which should be paid to original document\n",
    "    \"\"\"\n",
    "    differences_in_token_count = []\n",
    "    old_features = np.load(\"../data/features/\"+feature_fname+\".npy\") # tf-idf\n",
    "    new_features = []\n",
    "    for i in range(len(old_features)):\n",
    "        old_feat = old_features[i]\n",
    "        old_token_count = np.sum(old_feat!=0)\n",
    "        old_feat_neighbors = closest_neighbors[str(i)][0:k] # k closest neighbors\n",
    "        cos_sim_total = sum([tup[0] for tup in old_feat_neighbors]) # the denominator for scaling all k closest neighbors\n",
    "        \n",
    "        new_feat = np.zeros(old_feat.shape)\n",
    "        for cos_sim,data_index in old_feat_neighbors: # weighted sum of old features\n",
    "            new_feat += ((cos_sim/cos_sim_total)*old_features[data_index])\n",
    "        new_feat = (lam*old_feat)+((1-lam)*new_feat)\n",
    "        new_features.append(new_feat)\n",
    "        \n",
    "        new_token_count = np.sum(new_feat!=0)\n",
    "        diff_token_count = new_token_count-old_token_count\n",
    "        differences_in_token_count.append(diff_token_count)\n",
    "        \n",
    "    new_features = np.stack(new_features)\n",
    "    median_extra_tokens = round(np.median(differences_in_token_count),2)\n",
    "    mean_extra_tokens = round(np.average(differences_in_token_count),2)\n",
    "    return new_features,median_extra_tokens,mean_extra_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=10; lam=0.75; tfidf_7572\n",
      "(27.0, 40.77)\n",
      "-------------------------------------\n",
      "k=10; lam=0.75; tfidf_stacked_8_7572_10838\n",
      "(51.0, 74.35)\n",
      "-------------------------------------\n",
      "k=10; lam=0.75; tfidf_topics_7870\n",
      "(25.0, 37.14)\n",
      "-------------------------------------\n",
      "k=10; lam=0.5; tfidf_7572\n",
      "(27.0, 40.77)\n",
      "-------------------------------------\n",
      "k=10; lam=0.5; tfidf_stacked_8_7572_10838\n",
      "(51.0, 74.35)\n",
      "-------------------------------------\n",
      "k=10; lam=0.5; tfidf_topics_7870\n",
      "(25.0, 37.14)\n",
      "-------------------------------------\n",
      "k=10; lam=0.25; tfidf_7572\n",
      "(27.0, 40.77)\n",
      "-------------------------------------\n",
      "k=10; lam=0.25; tfidf_stacked_8_7572_10838\n",
      "(51.0, 74.35)\n",
      "-------------------------------------\n",
      "k=10; lam=0.25; tfidf_topics_7870\n",
      "(25.0, 37.14)\n",
      "-------------------------------------\n",
      "k=50; lam=0.75; tfidf_7572\n",
      "(140.0, 166.02)\n",
      "-------------------------------------\n",
      "k=50; lam=0.75; tfidf_stacked_8_7572_10838\n",
      "(251.0, 291.57)\n",
      "-------------------------------------\n",
      "k=50; lam=0.75; tfidf_topics_7870\n",
      "(125.0, 144.92)\n",
      "-------------------------------------\n",
      "k=50; lam=0.5; tfidf_7572\n",
      "(140.0, 166.02)\n",
      "-------------------------------------\n",
      "k=50; lam=0.5; tfidf_stacked_8_7572_10838\n",
      "(251.0, 291.57)\n",
      "-------------------------------------\n",
      "k=50; lam=0.5; tfidf_topics_7870\n",
      "(125.0, 144.92)\n",
      "-------------------------------------\n",
      "k=50; lam=0.25; tfidf_7572\n",
      "(140.0, 166.02)\n",
      "-------------------------------------\n",
      "k=50; lam=0.25; tfidf_stacked_8_7572_10838\n",
      "(251.0, 291.57)\n",
      "-------------------------------------\n",
      "k=50; lam=0.25; tfidf_topics_7870\n",
      "(125.0, 144.92)\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "features = [\"tfidf_7572\",\"tfidf_stacked_8_7572_10838\",\"tfidf_topics_7870\"] # different tf-idf features\n",
    "k_s = [10,50]\n",
    "lam_s = [0.75,0.5,0.25]\n",
    "\n",
    "for k in k_s:\n",
    "    for lam in lam_s:\n",
    "        for feature_name in features:\n",
    "            new_feature,median_extra_tokens,mean_extra_tokens = expand_features(feature_name,closest_neighbors,k=k,lam=lam)\n",
    "            np.save(\"../data/features/\"+feature_name+\"_k={}_lam={}\".format(k,lam)+\".npy\",new_feature)\n",
    "            print(\"k={}; lam={}; {}\".format(k,lam,feature_name))\n",
    "            print((median_extra_tokens,mean_extra_tokens))\n",
    "            print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
