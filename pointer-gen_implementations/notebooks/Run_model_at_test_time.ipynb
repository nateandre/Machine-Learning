{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the run-time generation of text\n",
    "\n",
    "-note that the model displayed here was not actually trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense,Bidirectional,LSTM,Input,RepeatVector,Activation,Softmax,Embedding,Dot,Lambda\n",
    "from tensorflow.keras.layers import Softmax,Concatenate\n",
    "from tensorflow.keras.layers import LayerNormalization # consider using layer norm. for the bidirectional encoder\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1,'../code/')\n",
    "from get_model_predictions import get_model_inputs,get_runtime_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading an example datapoint\n",
    "with open(\"../data/len_500_data/bill_information.json\") as in_file:\n",
    "    data_dict = json.load(in_file)\n",
    "data_point = data_dict['201520160AB1']\n",
    "summary = data_point['summary']\n",
    "utterances = data_point['utterances']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This bill prohibits a city or county from imposing a fine for a brown lawn or failure to water a lawn during a period for which the Governor has issued a state of emergency due to drought conditions.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Good morning, Mr. Chair and members. First of all, I want to say that I'm accepting the amendments your committee proposed and I want to thank your staff for working on this issue with me. Second, I'd like to add Senator Nielsen. He's reached out to me and he'd like to be added as a coauthor. So could your committee please add him?\",\n",
       " 'AB 1 is a simple bill that will prohibit municipalities from imposing a fine under any ordinance for failure to water your lawn or from having a brown lawn, during the period for which the governor has declared a drought emergency.',\n",
       " \"During this historic drought, this is really a modest measure. However, 80% of much of the water we use is the water grass, we can't eat. In one of my communities, there were people who were not watering their grass and they were fined.\",\n",
       " \"So this is a measure that says, we need to do something about that here at the state, so that we don't waste the water and I am so happy that I was able to get AB 1 because water is very important to the state of California at this time.\",\n",
       " \"Don't think it's named after me.\",\n",
       " \"Yes, I first brought this measure up last year, and my mayors in five cities each told me that they would not enforce those ordinances. Come year second, they started enforcing the ordinances. That's why it was necessary. So that's why I'm back here with this bill.\",\n",
       " \"It's enforceable. What is happened is we have one city in the inland empire where the city filed charges against a resident. The resident had to spend $4,000 to go to court and to remedy this issue. It started raining and since it started raining, the grass started coming up and they rather dropped it.\",\n",
       " \"But they wanted him to pay a $1,000 for all of their work that they put to go through the court. In another city, the code enforcement just started passing out letters saying that we're going to enforce this. If common sense would prevail, I wouldn't need this bill. I cannot believe that common sense doesn't prevail in every community.\",\n",
       " 'Oh sure.',\n",
       " 'Okay, we had a bill last year that was authored by Miss Campos and myself, coauthored by me and it was specifically for HOAs.',\n",
       " \"Yeah, they're not allowed to require green lawns during this emergency.\",\n",
       " \"They're not supposed to do that. That bill passed, the Governor signed it and it's been in law for a year.\",\n",
       " 'Good morning Mr. Speaker and colleagues. AB1 is a simple bill. It will prohibit municipalities from fining residents who choose to conserve water by not watering their lawns during the drought. It was passed out of local government Committee with a unanimous, bipartisan support.',\n",
       " \"This bill is necessary. I thought it wouldn't be necessary, but it's necessary because I have reports where municipalities have actually fined their residents for watering their grass. I respectfully ask for an aye vote.\",\n",
       " \"Thank you. I thank both of my colleagues for commenting on the bill. It's a very simple bill and I just hope that I can get your aye vote, thank you.\",\n",
       " 'Good morning, Mr. Speaker and members. AB 1 is back for concurrence, Senate amendments clarified this bill to apply to all cities including charter cities and added Senator Nielsen as a co-author. AB 1 is a simple bill. It will prohibit municipalities from fining residents who choose to conserve water by not watering their lawns during a drought emergency. I respectfully ask for your aye vote.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterances # 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 500) (1, 500) (1, 500) 500\n"
     ]
    }
   ],
   "source": [
    "## get the data representation necessary to feed to model\n",
    "x,x_indices,att_mask,x_indices_dict,index_to_word,all_tokens = get_model_inputs(utterances)\n",
    "print(x.shape,x_indices.shape,att_mask.shape,len(all_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good', 'morning', ',', 'mr.', 'chair', 'and', 'members', '.', 'first', 'of']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unchanged\n",
    "def apply_scatter_nd(updates,indices,tf_int,tf_float):\n",
    "    \"\"\" applies scatter_nd over the batch dimension\n",
    "    \"\"\"\n",
    "    out = Lambda(lambda entry: K.map_fn(lambda entry: tf.scatter_nd(entry[0],entry[1],tf.constant([30100],dtype=tf_int)),entry,dtype=tf_float))([indices,updates]) # assuming a max vocab_size+unique_words_in_input of 30000+100\n",
    "    return out\n",
    "\n",
    "# unchanged\n",
    "def apply_scatter_nd_add(tensor,updates,indices,tf_int,tf_float):\n",
    "    \"\"\" applies the tensor_scatter_nd_add over the batch dimension\n",
    "    \"\"\"\n",
    "    out = Lambda(lambda entry: K.map_fn(lambda entry: tf.tensor_scatter_nd_add(entry[0],entry[1],entry[2]),entry,dtype=tf_float))([tensor,indices,updates])\n",
    "    return out\n",
    "\n",
    "# changed\n",
    "def pointer_gen_encoder(embedding_layer,encoder_h=128,input_len=500,tf_int=tf.int32):\n",
    "    \"\"\" Returns the encoder portion of the pointer-gen network\n",
    "    \"\"\"\n",
    "    x = Input(shape=(input_len),dtype=tf_int) # input to the encoder\n",
    "    input_e = embedding_layer(x) # embeddings for the input\n",
    "    h = Bidirectional(LSTM(encoder_h,activation=\"tanh\",return_sequences=True),merge_mode=\"concat\")(input_e) # encoder\n",
    "    \n",
    "    model = Model(inputs=[x],outputs=[h])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changed - there is no more output_len\n",
    "def pointer_gen_decoder(embedding_layer,decoder_lstm,att_w1,att_w2,att_w3,att_v,vocab_d,vocab_d_pre,pgen_w1,pgen_w2,pgen_w3,encoder_h=128,input_len=500,tf_float=tf.float32,tf_int=tf.int32):\n",
    "    \"\"\" Returns the decoder portion of the pointer-gen network \n",
    "        -implemented so that it does only a single step\n",
    "    \"\"\"\n",
    "    h = Input(shape=(input_len,encoder_h*2),dtype=tf_float) # the input embedding from the encoder model\n",
    "    x_indices_ = Input(shape=(input_len),dtype=tf_int) # represents where each input word prob. should be added in joint prob. vector\n",
    "    x_indices = tf.expand_dims(x_indices_,axis=-1)\n",
    "    fixed_vocab_indices_ = Input(shape=(30000),dtype=tf_int) # the size of the input vocabulary\n",
    "    fixed_vocab_indices = tf.expand_dims(fixed_vocab_indices_,axis=-1)\n",
    "    att_mask = Input(shape=(input_len),dtype=tf_float) # mask used with the attention distribution to mask out padding\n",
    "    \n",
    "    decoder_x = Input(shape=(1),dtype=tf_int) # delayed y_data for input to the decoder (last prediction)\n",
    "    s_ = Input(shape=(256),dtype=tf_float) # decoder_h\n",
    "    c_ = Input(shape=(256),dtype=tf_float)\n",
    "    coverage_vector_ = Input(shape=(input_len),dtype=tf_float) # loaded at each step\n",
    "    s,c,coverage_vector = s_,c_,coverage_vector_\n",
    "    \n",
    "    decoder_e = embedding_layer(decoder_x) # embeddings for delayed input to the decoder\n",
    "    decoder_input = decoder_e[:,0,:]  # input to the decoder at this timestep\n",
    "    s,_,c = decoder_lstm(tf.expand_dims(decoder_input,axis=1),initial_state=[s,c])\n",
    "\n",
    "    # calculating attention (probabilities over input):\n",
    "    s_rep = RepeatVector(input_len)(s) # copying the decoder hidden state\n",
    "    e = att_v(Activation(\"tanh\")(att_w1(h)+att_w2(s_rep)+att_w3(tf.expand_dims(coverage_vector,axis=-1)))) # unscaled attention\n",
    "    e = tf.squeeze(e,axis=-1)+att_mask # using attention mask (masks out padding in the input sequence)\n",
    "    a = Activation(\"softmax\")(e) # scaled attention (represents prob. over input)\n",
    "\n",
    "    # handling coverage vector computations - note that coverage loss is not collected:\n",
    "    coverage_vector+=a\n",
    "\n",
    "    # calculating probabilities over fixed vocabulary:\n",
    "    context = Dot(axes=1)([a,h]) # calculating the context vector\n",
    "    pre_vocab_prob = Concatenate()([s,context])\n",
    "    pre_vocab_prob = vocab_d_pre(pre_vocab_prob) # extra Dense layer\n",
    "    pre_vocab_prob = vocab_d(pre_vocab_prob)\n",
    "    vocab_prob = Activation(\"softmax\")(pre_vocab_prob)\n",
    "\n",
    "    # calculation probabilty for text generation:\n",
    "    pre_gen_prob = pgen_w1(context)+pgen_w2(s)+pgen_w3(decoder_input)\n",
    "    gen_prob = Activation(\"sigmoid\")(pre_gen_prob)\n",
    "\n",
    "    # calculating joint-probability for generation/copying:\n",
    "    vocab_prob *= gen_prob # probability of generating a word from the fixed vocabulary\n",
    "    copy_prob = a*(1-gen_prob) # probability of copying a word from the input\n",
    "\n",
    "    # creating the joint-probability vector:\n",
    "    vocab_prob_projected = apply_scatter_nd(vocab_prob,fixed_vocab_indices,tf_int,tf_float)\n",
    "    joint_prob = apply_scatter_nd_add(vocab_prob_projected,copy_prob,x_indices,tf_int,tf_float)\n",
    "\n",
    "    model = Model(inputs=[h,x_indices_,decoder_x,att_mask,s_,c_,coverage_vector_,fixed_vocab_indices_],outputs=[joint_prob,s,c,coverage_vector])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pointer_gen_network(embedding_dim=100,input_len=500,tf_float=tf.float32,tf_int=tf.int32,model_save_path=\"../model_params/\"):\n",
    "    \"\"\" loads the encoder and decoder models from memory\n",
    "    args:\n",
    "        embedding_dim: the dimensionality of the word embeddings\n",
    "        model_save_path: directory which stores the saved model parameters\n",
    "    \"\"\"\n",
    "    embedding_layer = Embedding(input_dim=30000,output_dim=embedding_dim,mask_zero=True) # re-used for both the encoder and decoder\n",
    "    decoder_h=256\n",
    "    encoder_h=128\n",
    "    decoder_lstm = LSTM(decoder_h,activation=\"tanh\",return_state=True)\n",
    "    att_w1 = Dense(256,use_bias=True,activation=None)\n",
    "    att_w2 = Dense(256,use_bias=True,activation=None)\n",
    "    att_w3 = Dense(256,use_bias=True,activation=None) # should be 256x1 weight matrix\n",
    "    att_v = Dense(1,use_bias=False,activation=None)\n",
    "    vocab_d_pre = Dense(512,use_bias=True,activation=\"relu\") # an additional hidden layer before prediction vocab probs.\n",
    "    vocab_d = Dense(30000,use_bias=True,activation=None) # 30000 is fixed_vocabulary size\n",
    "    pgen_w1 = Dense(1,use_bias=True,activation=None)\n",
    "    pgen_w2 = Dense(1,use_bias=True,activation=None)\n",
    "    pgen_w3 = Dense(1,use_bias=True,activation=None)\n",
    "\n",
    "    encoder = pointer_gen_encoder(embedding_layer,encoder_h=encoder_h,input_len=input_len,tf_int=tf_int)\n",
    "    encoder.load_weights(model_save_path+\"encoder\")\n",
    "    decoder = pointer_gen_decoder(embedding_layer,decoder_lstm,att_w1,att_w2,att_w3,att_v,vocab_d,vocab_d_pre,pgen_w1,pgen_w2,pgen_w3,encoder_h=encoder_h,input_len=input_len,tf_float=tf_float,tf_int=tf_int)\n",
    "    decoder.load_weights(model_save_path+\"decoder\")\n",
    "    return encoder,decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder,decoder = get_pointer_gen_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['resident', 'resident', 'bill', 'bill', 'bill', 'bill', 'bill', 'bill', 'bill', 'bill']\n"
     ]
    }
   ],
   "source": [
    "### this is the implementation of beam search\n",
    "max_tokens = 10 #200 # maximum number of tokens for the generated summary\n",
    "alpha=1 # fully normalize by length\n",
    "beam_width=3\n",
    "vocab_size=len(index_to_word)\n",
    "\n",
    "## starting the decoding process:\n",
    "models = defaultdict(dict)\n",
    "\n",
    "s = np.zeros((1,256)).astype(\"float32\")\n",
    "c = np.zeros((1,256)).astype(\"float32\")\n",
    "coverage_vector = np.zeros((1,500)).astype(\"float32\")\n",
    "fixed_vocab_indices = np.array([[i for i in range(30000)]]).astype(\"int32\")\n",
    "decoder_x = np.ones((1,1)).astype(\"int32\") # represents first input of \"<SENT>\"\n",
    "\n",
    "h = encoder([x])\n",
    "joint_prob,s,c,coverage_vector = decoder([h,x_indices,decoder_x,att_mask,s,c,coverage_vector,fixed_vocab_indices])\n",
    "joint_prob = joint_prob.numpy()\n",
    "\n",
    "# getting the initial top n=beam_width models:\n",
    "for i in range(beam_width):\n",
    "    arg_max = np.argmax(joint_prob)\n",
    "    models[i]['prob']=np.log(joint_prob[0,arg_max]) # using log-prob.\n",
    "    if arg_max < vocab_size: # predicted word is in the fixed-vocabulary\n",
    "        models[i]['tokens']=[index_to_word[str(arg_max)]]\n",
    "        models[i]['next_input']=np.array([[arg_max]]).astype(\"int32\") # effectively the decoder_x\n",
    "    else: # predicting a word which is OOV but in the input\n",
    "        models[i]['tokens']=[x_indices_dict[arg_max]]\n",
    "        models[i]['next_input']=np.array([[2]]).astype(\"int32\") # represents the <UNK> token\n",
    "    \n",
    "    models[i]['s'],models[i]['c'],models[i]['coverage_vector']=s,c,coverage_vector\n",
    "    models[i]['done'] = (arg_max==1 or len(models[i]['tokens'])==max_tokens) # conditions for the end state\n",
    "    joint_prob[0,arg_max]=-np.inf\n",
    "    \n",
    "## run until the end condition is met for all n=beam_width models/outputs\n",
    "while sum([models[i]['done'] for i in range(beam_width)]) != beam_width:\n",
    "    \n",
    "    # first calculating all the new joint_probabilities for the n=beam_width models:\n",
    "    all_joint_probs = []\n",
    "    for i in range(beam_width):\n",
    "        if models[i]['done'] is False: # this model has not reached its end state; adding a new token at this step\n",
    "            s,c,coverage_vector,decoder_x = models[i]['s'],models[i]['c'],models[i]['coverage_vector'],models[i]['next_input']\n",
    "            joint_prob,s,c,coverage_vector = decoder([h,x_indices,decoder_x,att_mask,s,c,coverage_vector,fixed_vocab_indices])\n",
    "            joint_prob = (models[i]['prob']+np.log(joint_prob.numpy()))*(1/((len(models[i]['tokens'])+1)**alpha)) # normalization/scaling\n",
    "            models[i]['s'],models[i]['c'],models[i]['coverage_vector']=s,c,coverage_vector\n",
    "        else: # this model has already reached its end state; NOT adding a token at this state\n",
    "            joint_prob = np.full(joint_prob.shape,-np.inf).astype(\"float32\")\n",
    "            joint_prob[0,0]=models[i]['prob']*(1/(len(models[i]['tokens'])**alpha)) # only one cell will contain probability for this model (preventing the same \"done\" model from being selected multiple times); this simplifies the logic\n",
    "        all_joint_probs.append(joint_prob)\n",
    "\n",
    "    all_joint_probs = np.hstack(all_joint_probs)\n",
    "    \n",
    "    # based on the potential predicted sequences, getting the next n=beam_width best models:\n",
    "    new_models = defaultdict(dict) # dict to store the next best models\n",
    "    for i in range(beam_width): # getting the n=beam_width best paths\n",
    "        arg_max = np.argmax(all_joint_probs) # arg_max for the concatenation of all joint_prob arrays\n",
    "        model_no = arg_max // joint_prob.shape[1] # model associated with this argmax\n",
    "        \n",
    "        if models[model_no]['done'] is True: # highest prob. model is the finished model; simply copy eveything from the existing model\n",
    "            new_models[i]['s'],new_models[i]['c'],new_models[i]['coverage_vector']=models[model_no]['s'],models[model_no]['c'],models[model_no]['coverage_vector']\n",
    "            new_models[i]['prob'],new_models[i]['tokens'],new_models[i]['next_input'],new_models[i]['done']=models[model_no]['prob'],models[model_no]['tokens'],models[model_no]['next_input'],models[model_no]['done']\n",
    "            \n",
    "        else: # highest prob. model is not finished adding words/tokens\n",
    "            new_models[i]['prob']=all_joint_probs[0,arg_max]/(1/((len(models[model_no]['tokens'])+1)**alpha)) # getting rid of the scaling\n",
    "            model_arg_max = arg_max-(joint_prob.shape[1]*model_no) # arg_max for the joint_prob for this model\n",
    "            if model_arg_max < vocab_size: # predicted word is in the fixed-vocabulary\n",
    "                new_models[i]['tokens'] = models[model_no]['tokens']+[index_to_word[str(model_arg_max)]]\n",
    "                new_models[i]['next_input']=np.array([[model_arg_max]]).astype(\"int32\")\n",
    "            else: # predicting a word which is OOV but in the input\n",
    "                new_models[i]['tokens'] = models[model_no]['tokens']+[x_indices_dict[model_arg_max]]\n",
    "                new_models[i]['next_input']=np.array([[2]]).astype(\"int32\") # represents the <UNK> token\n",
    "                \n",
    "            new_models[i]['s'],new_models[i]['c'],new_models[i]['coverage_vector']=models[model_no]['s'],models[model_no]['c'],models[model_no]['coverage_vector']\n",
    "            new_models[i]['done'] = (model_arg_max==1 or len(new_models[i]['tokens'])==max_tokens)\n",
    "        \n",
    "        all_joint_probs[0,arg_max]=-np.inf\n",
    "    models = new_models\n",
    "    \n",
    "predicted_tokens = models[0]['tokens']\n",
    "print(predicted_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the final implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['resident', 'resident', 'bill', 'bill', 'bill', 'bill', 'bill', 'bill', 'bill', 'bill']\n"
     ]
    }
   ],
   "source": [
    "predicted_tokens = get_runtime_prediction(utterances,max_tokens=10)\n",
    "print(predicted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
