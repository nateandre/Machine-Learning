{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the pointer-gen model with this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense,Bidirectional,LSTM,Input,RepeatVector,Activation,Softmax,Embedding,Dot,Lambda\n",
    "from tensorflow.keras.layers import Softmax,Concatenate\n",
    "from tensorflow.keras.layers import LayerNormalization # consider using layer norm. for the bidirectional encoder\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load(\"../data/{}/x{}.npy\".format(\"len_500_data\",\"_500\"))\n",
    "x_indices = np.load(\"../data/{}/x_indices{}.npy\".format(\"len_500_data\",\"_500\"))\n",
    "att_mask = np.load(\"../data/{}/att_mask{}.npy\".format(\"len_500_data\",\"_500\"))\n",
    "loss_mask = np.load(\"../data/{}/loss_mask{}.npy\".format(\"len_500_data\",\"_500\"))\n",
    "decoder_x = np.load(\"../data/{}/decoder_x{}.npy\".format(\"len_500_data\",\"_500\"))\n",
    "y_indices = np.load(\"../data/{}/y_indices{}.npy\".format(\"len_500_data\",\"_500\"))\n",
    "embedding_matrix = np.load(\"../data/{}/word_embeddings.npy\".format(\"len_500_data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5900, 500) (5900, 500) (5900, 500)\n",
      "(5900, 101) (5900, 101) (5900, 101)\n",
      "(30000, 100)\n",
      "int32 int32 float32\n"
     ]
    }
   ],
   "source": [
    "print(x.shape,x_indices.shape,att_mask.shape)\n",
    "print(loss_mask.shape,decoder_x.shape,y_indices.shape)\n",
    "print(embedding_matrix.shape)\n",
    "print(x.dtype,x_indices.dtype,att_mask.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(prediction_probabilities,loss_mask,coverage_loss,lam,use_coverage_loss,return_indiv_loss=False):\n",
    "    \"\"\" Returns the loss for this batch - also allows for the returning of the loss value for the given input\n",
    "    args:\n",
    "        prediction_probabilities: model-assigned probabilities for ground-truth predictions\n",
    "        loss_mask: vector of 1s,0s specifying whether an input should contribute to the loss\n",
    "        coverage_loss: coverage loss for this batch of examples\n",
    "        lam: hyperparameter determining the contribution of coverage_loss to overall loss\n",
    "        use_coverage_loss: whether coverage loss should be used\n",
    "    \"\"\"\n",
    "    p_words = -tf.math.log(prediction_probabilities)\n",
    "    p_words *= loss_mask # applying the loss mask\n",
    "    p_words = tf.reduce_sum(p_words,axis=-1)\n",
    "    general_loss_component = tf.reduce_mean(p_words)\n",
    "    \n",
    "    # incorporating the coverage loss:\n",
    "    coverage_loss_component = 0\n",
    "    if use_coverage_loss:\n",
    "        coverage_loss *= loss_mask # applying the loss mask\n",
    "        coverage_loss = tf.reduce_sum(coverage_loss,axis=-1)\n",
    "        coverage_loss_component = lam*tf.reduce_mean(coverage_loss)\n",
    "        \n",
    "    total_loss = general_loss_component+coverage_loss_component\n",
    "    if return_indiv_loss:\n",
    "        indiv_losses = p_words\n",
    "        if use_coverage_loss:\n",
    "            indiv_losses+=coverage_loss\n",
    "        return total_loss,indiv_losses\n",
    "    else:\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_scatter_nd(updates,indices,tf_int,tf_float):\n",
    "    \"\"\" applies scatter_nd over the batch dimension\n",
    "    \"\"\"\n",
    "    out = Lambda(lambda entry: K.map_fn(lambda entry: tf.scatter_nd(entry[0],entry[1],tf.constant([30100],dtype=tf_int)),entry,dtype=tf_float))([indices,updates]) # assuming a max vocab_size+unique_words_in_input of 30000+100\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_scatter_nd_add(tensor,updates,indices,tf_int,tf_float):\n",
    "    \"\"\" applies the tensor_scatter_nd_add over the batch dimension\n",
    "    \"\"\"\n",
    "    out = Lambda(lambda entry: K.map_fn(lambda entry: tf.tensor_scatter_nd_add(entry[0],entry[1],entry[2]),entry,dtype=tf_float))([tensor,indices,updates])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointer_gen_encoder(embedding_layer,encoder_h=128,input_len=500,tf_int=tf.int32,use_dropout=False):\n",
    "    \"\"\" Returns the encoder portion of the pointer-gen network\n",
    "    \"\"\"\n",
    "    x = Input(shape=(input_len),dtype=tf_int) # input to the encoder\n",
    "    input_e = embedding_layer(x) # embeddings for the input\n",
    "    if use_dropout:\n",
    "        input_e = Dropout(0.25)(input_e)\n",
    "    h = Bidirectional(LSTM(encoder_h,activation=\"tanh\",return_sequences=True),merge_mode=\"concat\")(input_e) # encoder\n",
    "    \n",
    "    model = Model(inputs=[x],outputs=[h])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointer_gen_decoder(embedding_layer,decoder_lstm,att_w1,att_w2,att_w3,att_v,vocab_d,vocab_d_pre,pgen_w1,pgen_w2,pgen_w3,encoder_h=128,input_len=500,output_len=101,tf_float=tf.float32,tf_int=tf.int32):\n",
    "    \"\"\" Returns the decoder portion of the pointer-gen network\n",
    "    args:\n",
    "        input_len: the length of the input sequence (to the encoder)\n",
    "        output_len: the length of the output sequence (from the decoder)\n",
    "        tf_float,tf_int: defining datatypes for use in this model\n",
    "    \"\"\"\n",
    "    h = Input(shape=(input_len,encoder_h*2),dtype=tf_float) # the input embedding from the encoder model\n",
    "    x_indices_ = Input(shape=(input_len),dtype=tf_int) # represents where each input word prob. should be added in joint prob. vector\n",
    "    x_indices = tf.expand_dims(x_indices_,axis=-1)\n",
    "    fixed_vocab_indices_ = Input(shape=(30000),dtype=tf_int) # the size of the input vocabulary\n",
    "    fixed_vocab_indices = tf.expand_dims(fixed_vocab_indices_,axis=-1)\n",
    "    att_mask = Input(shape=(input_len),dtype=tf_float) # mask used with the attention distribution to mask out padding\n",
    "    decoder_x = Input(shape=(output_len),dtype=tf_int) # delayed y_data for input to the decoder (for teacher-forcing)\n",
    "    y_indices = Input(shape=(output_len),dtype=tf_int) # indices of the correct word in the joint_probabilities vector\n",
    "    s_ = Input(shape=(256),dtype=tf_float) # decoder_h\n",
    "    c_ = Input(shape=(256),dtype=tf_float)\n",
    "    coverage_vector_ = Input(shape=(input_len),dtype=tf_float)\n",
    "    s,c,coverage_vector = s_,c_,coverage_vector_\n",
    "    \n",
    "    decoder_e = embedding_layer(decoder_x) # embeddings for delayed input to the decoder\n",
    "    outputs = [] # stores probability of correct ground-truth predictions at each decoder output step\n",
    "    coverage_loss_contributions = [] # stores coverage loss contribution for each decoder output step\n",
    "    \n",
    "    for i in range(output_len): # loop through each step of the decoder\n",
    "        decoder_input = decoder_e[:,i,:]  # input to the decoder at this timestep\n",
    "        s,_,c = decoder_lstm(tf.expand_dims(decoder_input,axis=1),initial_state=[s,c])\n",
    "        \n",
    "        # calculating attention (probabilities over input):\n",
    "        s_rep = RepeatVector(input_len)(s) # copying the decoder hidden state\n",
    "        e = att_v(Activation(\"tanh\")(att_w1(h)+att_w2(s_rep)+att_w3(tf.expand_dims(coverage_vector,axis=-1)))) # unscaled attention\n",
    "        e = tf.squeeze(e,axis=-1)+att_mask # using attention mask (masks out padding in the input sequence)\n",
    "        a = Activation(\"softmax\")(e) # scaled attention (represents prob. over input)\n",
    "        \n",
    "        # handling coverage vector computations:\n",
    "        step_coverage_loss = tf.reduce_sum(tf.minimum(coverage_vector,a),axis=-1) # cov loss at this decoder step\n",
    "        coverage_loss_contributions.append(step_coverage_loss)\n",
    "        coverage_vector+=a\n",
    "        \n",
    "        # calculating probabilities over fixed vocabulary:\n",
    "        context = Dot(axes=1)([a,h]) # calculating the context vector\n",
    "        pre_vocab_prob = Concatenate()([s,context])\n",
    "        pre_vocab_prob = vocab_d_pre(pre_vocab_prob) # @@@ new\n",
    "        pre_vocab_prob = vocab_d(pre_vocab_prob)\n",
    "        vocab_prob = Activation(\"softmax\")(pre_vocab_prob)\n",
    "        \n",
    "        # calculation probabilty for text generation:\n",
    "        pre_gen_prob = pgen_w1(context)+pgen_w2(s)+pgen_w3(decoder_input)\n",
    "        gen_prob = Activation(\"sigmoid\")(pre_gen_prob)\n",
    "    \n",
    "        # calculating joint-probability for generation/copying:\n",
    "        vocab_prob *= gen_prob # probability of generating a word from the fixed vocabulary\n",
    "        copy_prob = a*(1-gen_prob) # probability of copying a word from the input\n",
    "        \n",
    "        # creating the joint-probability vector:\n",
    "        vocab_prob_projected = apply_scatter_nd(vocab_prob,fixed_vocab_indices,tf_int,tf_float)\n",
    "        joint_prob = apply_scatter_nd_add(vocab_prob_projected,copy_prob,x_indices,tf_int,tf_float)\n",
    "        \n",
    "        # gathering predictions from joint-probability vector - doing it here will reduce memory consumption\n",
    "        y_indices_i = tf.expand_dims(y_indices[:,i],axis=-1) # getting predictions at time i for whole batch\n",
    "        predictions_i = tf.squeeze(tf.gather(joint_prob,y_indices_i,batch_dims=1,axis=-1),axis=-1)\n",
    "        outputs.append(predictions_i)\n",
    "    \n",
    "    prediction_probabilities = K.permute_dimensions(tf.convert_to_tensor(outputs),(1,0))\n",
    "    coverage_loss_contributions = K.permute_dimensions(tf.convert_to_tensor(coverage_loss_contributions),(1,0))\n",
    "    \n",
    "    model = Model(inputs=[h,x_indices_,decoder_x,att_mask,y_indices,s_,c_,coverage_vector_,fixed_vocab_indices_],outputs=[prediction_probabilities,coverage_loss_contributions])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pointer_gen_network(embedding_matrix,embedding_dim=100,input_len=500,tf_float=tf.float32,tf_int=tf.int32,use_dropout=False):\n",
    "    \"\"\" initializes re-used model layers and creates the pointer-gen keras model object\n",
    "    args:\n",
    "        embedding_matrix: the matrix of pretrained weights\n",
    "        embedding_dim: the dimensionality of the word embeddings\n",
    "    \"\"\"\n",
    "    embedding_layer = Embedding(input_dim=30000,output_dim=embedding_dim,weights=[embedding_matrix],trainable=True,mask_zero=True) # re-used for both the encoder and decoder\n",
    "    decoder_h=256\n",
    "    encoder_h=128\n",
    "    decoder_lstm = LSTM(decoder_h,activation=\"tanh\",return_state=True)\n",
    "    att_w1 = Dense(256,use_bias=True,activation=None)\n",
    "    att_w2 = Dense(256,use_bias=True,activation=None)\n",
    "    att_w3 = Dense(256,use_bias=True,activation=None) # should be 256x1 weight matrix\n",
    "    att_v = Dense(1,use_bias=False,activation=None)\n",
    "    vocab_d_pre = Dense(512,use_bias=True,activation=\"relu\") # an additional hidden layer before prediction vocab probs.\n",
    "    vocab_d = Dense(30000,use_bias=True,activation=None) # 30000 is fixed_vocabulary size\n",
    "    pgen_w1 = Dense(1,use_bias=True,activation=None)\n",
    "    pgen_w2 = Dense(1,use_bias=True,activation=None)\n",
    "    pgen_w3 = Dense(1,use_bias=True,activation=None)\n",
    "\n",
    "    if use_dropout:\n",
    "        print(\"\\nUsing Dropout.\\n\")\n",
    "    \n",
    "    encoder = pointer_gen_encoder(embedding_layer,encoder_h=encoder_h,input_len=input_len,tf_int=tf_int,use_dropout=use_dropout)\n",
    "    decoder = pointer_gen_decoder(embedding_layer,decoder_lstm,att_w1,att_w2,att_w3,att_v,vocab_d,vocab_d_pre,pgen_w1,pgen_w2,pgen_w3,encoder_h=encoder_h,input_len=input_len,output_len=101,tf_float=tf_float,tf_int=tf_int)\n",
    "    return encoder,decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1801750858624775\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "## model load time is now invariant of the batch_size\n",
    "encoder,decoder = get_pointer_gen_network(embedding_matrix=embedding_matrix)\n",
    "end = time.time()-start\n",
    "print(end/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## improves speed by about 2x\n",
    "@tf.function\n",
    "def training_step(encoder,decoder,optimizer,x_subset,x_indices_subset,decoder_x_subset,att_mask_subset,y_indices_subset,loss_mask_subset,s_subset,c_subset,coverage_vector_subset,fixed_vocab_indices_subset,coverage_lam,use_coverage_loss):\n",
    "    \"\"\" training step - calculates the gradient w/ respect to encoder & decoder parameters\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        h = encoder(x_subset)\n",
    "        joint_probabilities,coverage_loss = decoder([h,x_indices_subset,decoder_x_subset,att_mask_subset,y_indices_subset,s_subset,c_subset,coverage_vector_subset,fixed_vocab_indices_subset])\n",
    "        loss = loss_function(joint_probabilities,loss_mask_subset,coverage_loss,lam=coverage_lam,use_coverage_loss=use_coverage_loss,return_indiv_loss=False)\n",
    "    \n",
    "    gradients = tape.gradient(loss, encoder.trainable_variables+decoder.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, encoder.trainable_variables+decoder.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.852092937628428\n",
      "0.44077348709106445\n",
      "385.1106872558594\n",
      "0.560200834274292\n",
      "0.5396111011505127\n",
      "271.8620834350586\n",
      "0.840312397480011\n",
      "0.4250098983446757\n",
      "213.15685272216797\n",
      "0.4803715507189433\n",
      "0.2869657874107361\n",
      "204.45901489257812\n",
      "0.30523216327031455\n",
      "0.25673330227533975\n",
      "198.3339385986328\n",
      "0.3519227186838786\n",
      "0.3227228164672852\n",
      "190.84307098388672\n",
      "0.31229798396428426\n",
      "0.2888719995816549\n",
      "184.23367309570312\n",
      "0.2701987346013387\n",
      "0.2473171353340149\n",
      "180.86798858642578\n",
      "0.3684409340222677\n",
      "0.29786616961161294\n",
      "176.99497985839844\n",
      "0.293757168451945\n",
      "0.24945653279622396\n",
      "173.8717803955078\n"
     ]
    }
   ],
   "source": [
    "batch_size=5\n",
    "coverage_lam = 0.0\n",
    "use_coverage_loss = False\n",
    "optimizer = Adam(lr=0.01)\n",
    "s_subset = np.zeros((batch_size,256)).astype(\"float32\")\n",
    "c_subset = np.zeros((batch_size,256)).astype(\"float32\")\n",
    "coverage_vector_subset = np.zeros((batch_size,500)).astype(\"float32\")\n",
    "fixed_vocab_indices_subset = np.vstack([[i for i in range(30000)] for _ in range(batch_size)]).astype(\"int32\")\n",
    "\n",
    "for _ in range(10): # epochs\n",
    "    losses = []\n",
    "    for i in range(0,10,batch_size): # only going thru 10 ex.\n",
    "        x_subset = x[i:i+batch_size]\n",
    "        x_indices_subset = x_indices[i:i+batch_size]\n",
    "        decoder_x_subset = decoder_x[i:i+batch_size]\n",
    "        att_mask_subset = att_mask[i:i+batch_size]\n",
    "        y_indices_subset = y_indices[i:i+batch_size]\n",
    "        loss_mask_subset = loss_mask[i:i+batch_size]\n",
    "        start = time.time()\n",
    "        batch_loss = training_step(encoder,decoder,optimizer,x_subset,x_indices_subset,decoder_x_subset,att_mask_subset,y_indices_subset,loss_mask_subset,s_subset,c_subset,coverage_vector_subset,fixed_vocab_indices_subset,coverage_lam,use_coverage_loss)\n",
    "        losses.append(float(batch_loss))\n",
    "        print((time.time()-start)/60)\n",
    "        \n",
    "    print(sum(losses)/max(len(losses),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
