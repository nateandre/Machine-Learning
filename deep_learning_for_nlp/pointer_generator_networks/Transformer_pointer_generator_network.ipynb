{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pointer-Generator Network for Title Generation with Transformer-encoder\n",
    "\n",
    "There are many variations of the Pointer-Generator network; this implementation was based on the following paper: Get To The Point: <i>Summarization with Pointer-Generator Networks</i> but using a [Transformer](https://arxiv.org/abs/1706.03762) encoder rather than a bi-directional RNN, as inspired by: <i>MS-Pointer Network: Abstractive Text Summary Based on Multi-Head Self-Attention</i>. The dataset used is a set of BBC business articles found on Kaggle.\n",
    "\n",
    "This makes a number of changes from the initial Vanilla implementation beyond the addition of the Transformer encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense,LSTM,Input,RepeatVector,Activation,Softmax,Embedding,Dot\n",
    "from tensorflow.keras.layers import Softmax,Concatenate,LayerNormalization,Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_cutoff=400 # this is the amount to pad up to for the input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting all of the data\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "data_dir = \"../../data/bbc_news_summary/news_articles/business/\"\n",
    "files = os.listdir(data_dir)\n",
    "\n",
    "headlines = [] # max length for headlines is 11 + 1 (for the <s>)\n",
    "body_texts = [] # max length for body text is 400 (imposed)\n",
    "all_texts = []\n",
    "for fname in files:\n",
    "    with open(data_dir+fname) as data_file:\n",
    "        lines = data_file.readlines()\n",
    "        lines = [line.strip() for line in lines]\n",
    "        lines = [[tok.text.lower() for tok in nlp(line)] for line in lines]\n",
    "        headline = lines[0]\n",
    "        body = []\n",
    "        for line in lines[1:]:\n",
    "            body += line\n",
    "        body = body[:token_cutoff] # cutting off the length of the body text\n",
    "        headlines.append(headline)\n",
    "        body_texts.append(body)\n",
    "        all_texts += body+headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11727 4945 6782\n",
      "['the', '.', ',', 'to', 'of', 'in']\n",
      "['reinforce', 'sufficiently', 'warming', 'chairmanship', 'jacques', 'thabo']\n"
     ]
    }
   ],
   "source": [
    "# getting words which will be part of the fixed_vocabulary (words which appear >= 3 times)\n",
    "word_freq = Counter(all_texts) # there are 11,727 unique words\n",
    "words_by_freq = (list(word_freq.items()))\n",
    "words_by_freq.sort(key=lambda x: x[1],reverse=True) # smaller indices will correspond with more common words\n",
    "most_freq_words = [word_tup[0] for word_tup in words_by_freq if word_tup[1] >= 3] # 4945 words\n",
    "less_freq_words = [word_tup[0] for word_tup in words_by_freq if word_tup[1] < 3] # 6782 words\n",
    "print(len(word_freq),len(most_freq_words),len(less_freq_words))\n",
    "print(most_freq_words[0:6])\n",
    "print(less_freq_words[0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4948"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assigning indices for all words, and adding <PAD>,<SENT>,<UNK> symbols\n",
    "# <UNK> both for inputing words not in fixed_vocab and predicting words not in fixed_vocab or in input sequence\n",
    "word_to_index = {\"<PAD>\":0,\"<SENT>\":1,\"<UNK>\":2} # for all words (including less frequent words)\n",
    "index_to_word = {0:\"<PAD>\",1:\"<SENT>\",2:\"<UNK>\"}\n",
    "\n",
    "fixed_vocab_word_to_index = {\"<PAD>\":0,\"<SENT>\":1,\"<UNK>\":2} # for words assigned to the fixed_vocabulary\n",
    "fixed_vocab_index_to_word = {0:\"<PAD>\",1:\"<SENT>\",2:\"<UNK>\"}\n",
    "\n",
    "index = 3 # starting index for all words\n",
    "# assigning indices to most common words:\n",
    "for word in most_freq_words: \n",
    "    word_to_index[word]=index\n",
    "    index_to_word[index]=word\n",
    "    fixed_vocab_word_to_index[word]=index\n",
    "    fixed_vocab_index_to_word[index]=word\n",
    "    index += 1\n",
    "    \n",
    "# assigning indices to least common words:\n",
    "for word in less_freq_words:\n",
    "    word_to_index[word]=index\n",
    "    index_to_word[index]=word\n",
    "    index += 1\n",
    "\n",
    "len(fixed_vocab_word_to_index) # there are 4948 words in the fixed_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the input data representations for the model - input is padded up to a length of token_cutoff\n",
    "x = [] # stores the integer/index representation for all input\n",
    "x_indices = [] # stores the joint probability vector indices for all words in the input \n",
    "x_indices_dicts = [] # stores the dicts for assigning words which are not in the fixed_vocabulary\n",
    "att_mask = [] # stores the attention masks (0 for valid words, -np.inf for padding)\n",
    "\n",
    "\n",
    "for body_text in body_texts: # processing the input\n",
    "    x_rep = []\n",
    "    for token in body_text:\n",
    "        if token in fixed_vocab_word_to_index:\n",
    "            x_rep.append(fixed_vocab_word_to_index[token])\n",
    "        else:\n",
    "            x_rep.append(fixed_vocab_word_to_index['<UNK>'])\n",
    "    \n",
    "    att_mask_rep = [0 for i in range(len(x_rep))]\n",
    "    amount_to_pad = token_cutoff-len(x_rep)\n",
    "    x_rep += [0 for i in range(amount_to_pad)] # padding the input\n",
    "    att_mask_rep += [-np.inf for i in range(amount_to_pad)]\n",
    "    x.append(x_rep)\n",
    "    att_mask.append(att_mask_rep)\n",
    "    \n",
    "    index = 4948 # starting index for assignment to joint_probability vector\n",
    "    non_vocab_dict = {}\n",
    "    this_x_indices = []\n",
    "    for token in body_text: # assigning each word an index in the joint_probability vector\n",
    "        if token in fixed_vocab_word_to_index:\n",
    "            this_x_indices.append(fixed_vocab_word_to_index[token])\n",
    "        else:\n",
    "            if token in non_vocab_dict: # this word if OOV but has been seen before\n",
    "                this_x_indices.append(non_vocab_dict[token])\n",
    "            else: # this word is OOV and has never been seen before\n",
    "                non_vocab_dict[token]=index\n",
    "                this_x_indices.append(index)\n",
    "                index += 1\n",
    "                \n",
    "    x_indices_dicts.append(non_vocab_dict)\n",
    "    this_x_indices += [0 for i in range(amount_to_pad)] # padding will be masked out in att calculation, so padding with 0 here is valid\n",
    "    x_indices.append(this_x_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the largest number of OOV words for a given bid utterances\n",
    "max([len(dic) for dic in x_indices_dicts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the output representations for the model - output is padded up to a length of 11+1 (for final <s> prediction)\n",
    "## all words in output that are not in input utterances or in fixed_vocab_vector are assigned 3:<UNK>\n",
    "y = [] # stores the index representations for all words in the headlines\n",
    "loss_mask = [] # 1 for valid words, 0 for padding\n",
    "decoder_x = [] # starts with 1:<SENT>, followed by y[0:len(headline)-1] (this is the input for teacher-forcing)(12x1)\n",
    "y_indices = [] # index for the correct decoder prediction, in the joint-probability vector\n",
    "\n",
    "for hl_i,headline in enumerate(headlines): # processing the output\n",
    "    \n",
    "    y_rep = [] # not used in the model, stores indices using only fixed_vocab_vector\n",
    "    for token in headline:\n",
    "        if token in fixed_vocab_word_to_index:\n",
    "            y_rep.append(fixed_vocab_word_to_index[token])\n",
    "        else:\n",
    "            y_rep.append(fixed_vocab_word_to_index['<UNK>'])\n",
    "    y_rep.append(fixed_vocab_word_to_index['<SENT>']) # end delimiter of output representation\n",
    "    \n",
    "    loss_mask_rep = [1 for i in range(len(y_rep))]\n",
    "    decoder_x_rep = [1]+y_rep[0:len(y_rep)-1] # embedding word in input but not in fixed_vocab is currently set to <UNK>\n",
    "    amount_to_pad = 12-len(y_rep) # 11+1 represents final <SENT> prediction\n",
    "    y_rep += [0 for i in range(amount_to_pad)]\n",
    "    loss_mask_rep += [0 for i in range(amount_to_pad)] # cancels out loss contribution from padding\n",
    "    decoder_x_rep += [0 for i in range(amount_to_pad)]\n",
    "    \n",
    "    # creating joint-probability representation of output:\n",
    "    non_vocab_dict = x_indices_dicts[hl_i]\n",
    "    y_indices_rep = []\n",
    "    for token in headline:\n",
    "        if token in fixed_vocab_word_to_index: # word is in fixed_vocabulary\n",
    "            y_indices_rep.append(fixed_vocab_word_to_index[token])\n",
    "        elif token in non_vocab_dict: # word is OOV but in the input utterances, use the index assigned to this word in x_indices\n",
    "            y_indices_rep.append(non_vocab_dict[token])\n",
    "        else: # word is OOV and not in input utterances\n",
    "            y_indices_rep.append(fixed_vocab_word_to_index[\"<UNK>\"])\n",
    "    \n",
    "    y_indices_rep.append(fixed_vocab_word_to_index['<SENT>']) # last prediction should be <SENT>\n",
    "    y_indices_rep += [0 for i in range(amount_to_pad)] # padding ignored due to loss_mask\n",
    "    y.append(y_rep)\n",
    "    loss_mask.append(loss_mask_rep)\n",
    "    decoder_x.append(decoder_x_rep)\n",
    "    y_indices.append(y_indices_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(510, 400) (510, 400) (510, 400)\n",
      "(510, 12) (510, 12) (510, 12)\n"
     ]
    }
   ],
   "source": [
    "x = np.array(x)\n",
    "x_indices = np.array(x_indices)\n",
    "att_mask = np.array(att_mask)\n",
    "loss_mask = np.array(loss_mask)\n",
    "decoder_x = np.array(decoder_x)\n",
    "y_indices = np.array(y_indices)\n",
    "print(x.shape,x_indices.shape,att_mask.shape) \n",
    "print(loss_mask.shape,decoder_x.shape,y_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.astype(\"int64\")\n",
    "x_indices = x_indices.astype(\"int64\")\n",
    "att_mask = att_mask.astype(\"float64\")\n",
    "loss_mask = loss_mask.astype(\"int64\")\n",
    "decoder_x = decoder_x.astype(\"int64\")\n",
    "y_indices = y_indices.astype(\"int64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer code - including positional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position_vectors(num_positions,batch_size,vector_size=128,embedding_dtype=\"float64\"):\n",
    "    \"\"\" returns position vectors of shape:(num_positions,vector_size)\n",
    "    args:\n",
    "        num_positions: length of the input\n",
    "        batch_size: number of batches\n",
    "    \"\"\"\n",
    "    position_embeddings = []\n",
    "    positions = [i for i in range(num_positions)]\n",
    "    d=vector_size # the vector size\n",
    "    \n",
    "    for pos in positions: # creating an embedding for each item in sequence\n",
    "        emb = []\n",
    "        for i in range(0,d//2):\n",
    "            emb.append(math.sin(pos/(10000**(2*i/d))))\n",
    "            emb.append(math.cos(pos/(10000**(2*i/d))))\n",
    "        emb = np.array(emb)\n",
    "        position_embeddings.append(emb)\n",
    "    \n",
    "    position_embeddings = np.array(position_embeddings)\n",
    "    position_embeddings = position_embeddings.astype(embedding_dtype)\n",
    "    \n",
    "    batch_position_embeddings = [position_embeddings for _ in range(batch_size)]\n",
    "    batch_position_embeddings = np.array(batch_position_embeddings)\n",
    "    return batch_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 10, 128)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_embeddings = get_position_vectors(num_positions=10,batch_size=2)\n",
    "position_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_layer(Q_x,KV_x,mask=None):\n",
    "    \"\"\" Individual attention block for the encoder (dim=64)\n",
    "    args:\n",
    "        Q_x: input to caclulate the Q matrix (differs from KV_x in encoder-decoder attention block)\n",
    "        KV_x: input to calculate the K and V matrices\n",
    "        mask: masking for the decoder attention block\n",
    "    \"\"\"\n",
    "    # Dense layers w/ no bias&activation are equivalent to linear transformations:\n",
    "    Q = Dense(64,use_bias=False,activation=None)(Q_x) # queries\n",
    "    K = Dense(64,use_bias=False,activation=None)(KV_x) # keys\n",
    "    V = Dense(64,use_bias=False,activation=None)(KV_x) # values\n",
    "    \n",
    "    unscaled_att_weights = Dot(axes=-1)([Q,K])/tf.cast(tf.sqrt(64.0),tf.float64)\n",
    "    if mask is not None: # only for the decoder layer\n",
    "        unscaled_att_weights += mask\n",
    "    \n",
    "    att_weights = tf.nn.softmax(unscaled_att_weights,axis=-1)\n",
    "    att_output = tf.matmul(att_weights,V)\n",
    "    return att_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block(x,h=4,linear_projection=False):\n",
    "    \"\"\" Encoder block; num_attention_heads=4\n",
    "    args:\n",
    "        h: number of attention heads\n",
    "        linear_projection (bool): whether to linear project the input to same size as output of attention layer\n",
    "                                  this is only necessary for the first block if input dim != 256\n",
    "    \"\"\"\n",
    "    # multi-head attention:\n",
    "    attention_heads=[]\n",
    "    for _ in range(h):\n",
    "        att_output = attention_layer(x,x,mask=None)\n",
    "        attention_heads.append(att_output)\n",
    "    \n",
    "    multi_head_att_output = Concatenate()(attention_heads)\n",
    "    multi_head_att_output = Dense(256,use_bias=False,activation=None)(multi_head_att_output)\n",
    "    if linear_projection is True:\n",
    "        x = Dense(256,use_bias=False,activation=None)(x) # linear projection of input into higher dim. space\n",
    "    attention_output = LayerNormalization()(multi_head_att_output+x) # residual block 1\n",
    "    \n",
    "    # feed-forward:\n",
    "    ffn = Dense(512,activation='relu')(attention_output)\n",
    "    ffn = Dense(256,activation=None)(ffn)\n",
    "    encoder_output = LayerNormalization()(attention_output+ffn) # residual block 2\n",
    "    return encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(layer_input,num_blocks=4,num_heads=4):\n",
    "    \"\"\" stacks a number of transformer encoder blocks together ; currently uses a fixed 256 dim.\n",
    "    args:\n",
    "        layer_input: embedding input to the encoder\n",
    "        num_heads: number of attention heads\n",
    "        num_blocks: number of Transformer blocks \n",
    "    \"\"\"\n",
    "    layer_input = encoder_block(layer_input,h=num_heads,linear_projection=True)\n",
    "    \n",
    "    for _ in range(num_blocks-1):\n",
    "        layer_input = encoder_block(layer_input,h=num_heads,linear_projection=False)\n",
    "    return layer_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointer-gen architecture code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_scatter_nd(updates,indices,batch_size):\n",
    "    \"\"\" applies scatter_nd over the batch dimension\n",
    "    \"\"\"\n",
    "    return tf.convert_to_tensor([tf.scatter_nd(indices[i],updates[i],tf.constant([5100],dtype=tf.int64)) for i in range(batch_size)]) # assuming a max vocab_size+unique_words_in_input of 4948+102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointer_gen_network(embedding_layer,att_w1,att_w2,att_w3,att_v,vocab_d,pgen_w1,pgen_w2,pgen_w3,decoder_lstm,encoder_h=128,input_len=400,output_len=12,batch_size=30):\n",
    "    \"\"\" Returns pointer generator network using Transformer encoder\n",
    "    args:\n",
    "        input_len: the length of the input sequence (to the encoder)\n",
    "        output_len: the length of the output sequence (from the decoder)\n",
    "        batch_size: cannot be inferred so must be explicitly inputted\n",
    "    \"\"\"\n",
    "    x = Input(shape=(input_len),dtype=tf.int64) # input to the encoder\n",
    "    x_indices_ = Input(shape=(input_len),dtype=tf.int64) # represents where each input word prob. should be added in joint prob. vector\n",
    "    x_indices = tf.expand_dims(x_indices_,axis=-1)\n",
    "    att_mask = Input(shape=(input_len)) # mask used with the attention distribution to mask out padding\n",
    "    decoder_x = Input(shape=(output_len),dtype=tf.int64) # delayed y_data for input to the decoder (for teacher-forcing)\n",
    "    position_emb = Input(shape=(128)) # for Transformer encoder\n",
    "    y_indices = Input(shape=(output_len),dtype=tf.int64) # indices of the correct word in the joint_probabilities vector\n",
    "    s = tf.zeros((batch_size,256),dtype=tf.float64) # defining using batch_size makes model brittle, but fine for training\n",
    "    c = tf.zeros((batch_size,256),dtype=tf.float64)\n",
    "    coverage_vector = tf.zeros((batch_size,input_len),dtype=tf.float64)\n",
    "    \n",
    "    input_e = embedding_layer(x) #+position_emb # embeddings for the input, included position vectors\n",
    "    h = transformer_encoder(input_e,num_blocks=4,num_heads=8) # encoder\n",
    "    \n",
    "    decoder_e = embedding_layer(decoder_x) # embeddings for delayed input to the decoder\n",
    "    outputs = []\n",
    "    coverage_loss_contributions = [] # stores coverage loss contribution for each decoder output step\n",
    "    \n",
    "    for i in range(output_len): # loop through each step of the decoder\n",
    "        decoder_input = decoder_e[:,i,:]  # input to the decoder at this timestep\n",
    "        s,_,c = decoder_lstm(tf.expand_dims(decoder_input,axis=1),initial_state=[s,c])\n",
    "        \n",
    "        # calculating attention (probabilities over input):\n",
    "        s_rep = RepeatVector(input_len)(s) # copying the decoder hidden state\n",
    "        e = att_v(Activation(\"tanh\")(att_w1(h)+att_w2(s_rep)+att_w3(tf.expand_dims(coverage_vector,axis=-1)))) # unscaled attention\n",
    "        e = tf.squeeze(e,axis=-1)+att_mask # using attention mask (masks out padding in the input sequence)\n",
    "        a = Activation(\"softmax\")(e) # scaled attention (represents prob. over input)\n",
    "        \n",
    "        # handling coverage vector computations:\n",
    "        step_coverage_loss = tf.reduce_sum(tf.minimum(coverage_vector,a),axis=-1) # cov loss at this decoder step\n",
    "        coverage_loss_contributions.append(step_coverage_loss)\n",
    "        coverage_vector+=a\n",
    "        \n",
    "        # calculating probabilities over fixed vocabulary:\n",
    "        context = Dot(axes=1)([a,h]) # calculating the context vector\n",
    "        pre_vocab_prob = Concatenate()([s,context])\n",
    "        pre_vocab_prob = vocab_d(pre_vocab_prob)\n",
    "        vocab_prob = Activation(\"softmax\")(pre_vocab_prob)\n",
    "        \n",
    "        # calculation probabilty for text generation:\n",
    "        pre_gen_prob = pgen_w1(context)+pgen_w2(s)+pgen_w3(decoder_input)\n",
    "        gen_prob = Activation(\"sigmoid\")(pre_gen_prob)\n",
    "    \n",
    "        # calculating joint-probability for generation/copying:\n",
    "        vocab_prob *= gen_prob # probability of generating a word from the fixed vocabulary\n",
    "        copy_prob = a*(1-gen_prob) # probability of copying a word from the input\n",
    "        \n",
    "        fixed_vocab_indices = tf.tile(tf.reshape(tf.range(4948,dtype=tf.int64),(1,4948,1)),tf.constant([batch_size,1,1])) # 4948 is fixed_vocab size\n",
    "        vocab_prob_projected = apply_scatter_nd(vocab_prob,fixed_vocab_indices,batch_size)\n",
    "        copy_prob_projected = apply_scatter_nd(copy_prob,x_indices,batch_size)\n",
    "        joint_prob = vocab_prob_projected+copy_prob_projected\n",
    "        \n",
    "        # gathering predictions from joint-probability vector - doing it here will reduce memory consumption\n",
    "        y_indices_i = tf.expand_dims(y_indices[:,i],axis=-1) # getting predictions at time i for whole batch\n",
    "        predictions_i = tf.squeeze(tf.gather(joint_prob,y_indices_i,batch_dims=1,axis=-1),axis=-1)\n",
    "        outputs.append(predictions_i)\n",
    "    \n",
    "    outputs = K.permute_dimensions(tf.convert_to_tensor(outputs),(1,0))\n",
    "    coverage_loss_contributions = K.permute_dimensions(tf.convert_to_tensor(coverage_loss_contributions),(1,0))\n",
    "    \n",
    "    model = Model(inputs=[x,x_indices_,decoder_x,att_mask,position_emb,y_indices],outputs=[outputs,coverage_loss_contributions])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(prediction_probabilities,loss_mask,coverage_loss,lam=0.1,use_coverage_loss=True):\n",
    "    \"\"\" Returns the loss for this batch\n",
    "    args:\n",
    "        prediction_probabilities: model-assigned probabilities for ground-truth predictions\n",
    "        loss_mask: vector of 1s,0s specifying whether an input should contribute to the loss\n",
    "        coverage_loss: coverage loss for this batch of examples\n",
    "        lam: hyperparameter determining the contribution of coverage_loss to overall loss\n",
    "        use_coverage_loss: whether coverage loss should be used\n",
    "    \"\"\"\n",
    "    p_words = -tf.log(prediction_probabilities)\n",
    "    p_words *= loss_mask # applying the loss mask\n",
    "    p_words = tf.reduce_sum(p_words,axis=-1)\n",
    "    general_loss_component = tf.reduce_mean(p_words)\n",
    "    \n",
    "    # incorporating the coverage loss:\n",
    "    coverage_loss_component = 0\n",
    "    if use_coverage_loss:\n",
    "        coverage_loss *= loss_mask # applying the loss mask\n",
    "        coverage_loss = tf.reduce_sum(coverage_loss,axis=-1)\n",
    "        coverage_loss_component = lam*tf.reduce_mean(coverage_loss)\n",
    "        \n",
    "    total_loss = general_loss_component+coverage_loss_component\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(input_dim=4950,output_dim=128,mask_zero=False) # re-used for both the encoder and decoder\n",
    "decoder_h=256\n",
    "decoder_lstm = LSTM(decoder_h,activation=\"tanh\",return_state=True)\n",
    "att_w1 = Dense(256,use_bias=True,activation=None)\n",
    "att_w2 = Dense(256,use_bias=True,activation=None)\n",
    "att_w3 = Dense(256,use_bias=True,activation=None) # should be 256x1 weight matrix\n",
    "att_v = Dense(1,use_bias=False,activation=None)\n",
    "vocab_d = Dense(4948,use_bias=True,activation=None) # 4948 is fixed_vocabulary size\n",
    "pgen_w1 = Dense(1,use_bias=True,activation=None)\n",
    "pgen_w2 = Dense(1,use_bias=True,activation=None)\n",
    "pgen_w3 = Dense(1,use_bias=True,activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size=10\n",
    "model = pointer_gen_network(embedding_layer,att_w1,att_w2,att_w3,att_v,vocab_d,pgen_w1,pgen_w2,pgen_w3,decoder_lstm,encoder_h=128,input_len=400,output_len=12,batch_size=batch_size)\n",
    "optimizer = Adam(lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "56.70661272437699\n",
      "35.886144386268114\n",
      "33.451341102667115\n",
      "30.50584637617631\n",
      "28.827946096163487\n",
      "27.530733002281696\n",
      "26.597426506783624\n",
      "24.59784669256105\n",
      "23.415458865324652\n",
      "21.352970672037387\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "position_vector = get_position_vectors(num_positions=400,batch_size=batch_size,vector_size=128)\n",
    "\n",
    "for _ in range(10): # epochs\n",
    "    losses = []\n",
    "    for i in range(0,100-batch_size,batch_size): # only using first 100 samples for training\n",
    "        x_subset = x[i:i+batch_size]\n",
    "        x_indices_subset = x_indices[i:i+batch_size]\n",
    "        decoder_x_subset = decoder_x[i:i+batch_size]\n",
    "        att_mask_subset = att_mask[i:i+batch_size]\n",
    "        y_indices_subset = y_indices[i:i+batch_size]\n",
    "        loss_mask_subset = loss_mask[i:i+batch_size]\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction_probabilities,coverage_loss = model([x_subset,x_indices_subset,decoder_x_subset,att_mask_subset,position_vector,y_indices_subset])\n",
    "            loss = loss_function(prediction_probabilities,loss_mask_subset,coverage_loss,lam=0.1,use_coverage_loss=True)\n",
    "        losses.append(float(loss))\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    print(sum(losses)/max(len(losses),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
