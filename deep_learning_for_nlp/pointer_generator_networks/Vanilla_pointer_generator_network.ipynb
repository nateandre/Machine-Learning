{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pointer-Generator Network for Title Generation\n",
    "\n",
    "There are many variations of the Pointer-Generator network; this implementation was based on the following paper: <i>Get To The Point: Summarization with Pointer-Generator Networks</i>. The dataset used is a set of BBC business articles found on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense,Bidirectional,LSTM,Input,RepeatVector,Activation,Softmax,Embedding,Dot\n",
    "from tensorflow.keras.layers import Softmax,Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "import numpy as np\n",
    "import os\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting all of the data\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "data_dir = \"../data/bbc_news_summary/news_articles/business/\"\n",
    "files = os.listdir(data_dir)\n",
    "\n",
    "headlines = [] # max length for headlines is 11\n",
    "body_texts = [] # max length for body text is 400 (imposed)\n",
    "all_texts = []\n",
    "for fname in files:\n",
    "    with open(data_dir+fname) as data_file:\n",
    "        lines = data_file.readlines()\n",
    "        lines = [line.strip() for line in lines]\n",
    "        lines = [[tok.text.lower() for tok in nlp(line)] for line in lines]\n",
    "        headline = lines[0]\n",
    "        body = []\n",
    "        for line in lines[1:]:\n",
    "            body += line\n",
    "        body = body[:400] # cutting off the length of the body text\n",
    "        headlines.append(headline)\n",
    "        body_texts.append(body)\n",
    "        all_texts += body+headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', '.', ',', 'to', 'of', 'in']\n",
      "['reinforce', 'sufficiently', 'warming', 'chairmanship', 'jacques', 'thabo']\n"
     ]
    }
   ],
   "source": [
    "# getting words which will be part of the fixed_vocabulary\n",
    "## in this case, selecting the words which appear >= 3 times\n",
    "word_freq = Counter(all_texts) # there are 11,728 unique words\n",
    "words_by_freq = (list(word_freq.items()))\n",
    "words_by_freq.sort(key=lambda x: x[1],reverse=True) # smaller indices will correspond with more common words\n",
    "most_freq_words = [word_tup[0] for word_tup in words_by_freq if word_tup[1] >= 3] # 4945 words\n",
    "less_freq_words = [word_tup[0] for word_tup in words_by_freq if word_tup[1] < 3] # 6783 words\n",
    "print(most_freq_words[0:6])\n",
    "print(less_freq_words[0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4947"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assigning indices per word, with the first 4945+2 being associated with the fixed vocabulary\n",
    "## the +2 incorporates the 0:<PAD> token and the 1:<SENT> token\n",
    "word_to_index = {\"<PAD>\":0,\"<SENT>\":1} # for all words\n",
    "index_to_word = {0:\"<PAD>\",1:\"<SENT>\"}\n",
    "\n",
    "fixed_vocab_word_to_index = {\"<PAD>\":0,\"<SENT>\":1} # for words assigned to the fixed_vocabulary\n",
    "fixed_vocab_index_to_word = {0:\"<PAD>\",1:\"<SENT>\"}\n",
    "\n",
    "index = 2\n",
    "for word in most_freq_words: # assigning indices to most common words\n",
    "    word_to_index[word]=index\n",
    "    index_to_word[index]=word\n",
    "    fixed_vocab_word_to_index[word]=index\n",
    "    fixed_vocab_index_to_word[index]=word\n",
    "    index += 1\n",
    "    \n",
    "for word in less_freq_words: # assigning indices to least common words\n",
    "    word_to_index[word]=index\n",
    "    index_to_word[index]=word\n",
    "    index += 1\n",
    "    \n",
    "word_to_index[\"<UNK>\"] = index # words that are never seen before assigned to this index\n",
    "index_to_word[index] = \"<UNK>\"\n",
    "\n",
    "len(fixed_vocab_word_to_index) # there are 4947 words in the fixed_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating the input data representations for the model\n",
    "x = [] # stores the integer/index representation for all input\n",
    "x_indices = [] # stores the joint probability vector indices for all words in the input \n",
    "x_indices_dicts = [] # stores the dicts for assigning words which are not \n",
    "att_mask = [] # stores the attention masks (0 for valid words, -np.inf for padding)\n",
    "\n",
    "for body_text in body_texts: # processing the input\n",
    "    x_rep = [word_to_index[word] for word in body_text]\n",
    "    att_mask_rep = [0 for i in range(len(x_rep))]\n",
    "    amount_to_pad = 400-len(x_rep)\n",
    "    x_rep += [0 for i in range(amount_to_pad)] # padding the input\n",
    "    att_mask_rep += [-np.inf for i in range(amount_to_pad)]\n",
    "    x.append(x_rep)\n",
    "    att_mask.append(att_mask_rep)\n",
    "    \n",
    "    index = 4947 # starting index for assignment to joint_probability vector\n",
    "    non_vocab_dict = {}\n",
    "    this_x_indices = []\n",
    "    for word in body_text: # assigning each word an index in the joint_probability vector\n",
    "        if word in fixed_vocab_word_to_index:\n",
    "            this_x_indices.append(fixed_vocab_word_to_index[word])\n",
    "        else:\n",
    "            if word in non_vocab_dict: # this word if OOV but has been seen before\n",
    "                this_x_indices.append(non_vocab_dict[word])\n",
    "            else: # this word has never been seen before\n",
    "                non_vocab_dict[word]=index\n",
    "                this_x_indices.append(index)\n",
    "                index += 1\n",
    "                \n",
    "    x_indices_dicts.append(non_vocab_dict)\n",
    "    this_x_indices += [0 for i in range(amount_to_pad)] # padding will be masked out in att calculation, so padding with 0 here is valid\n",
    "    x_indices.append(this_x_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the output data representations for the model\n",
    "y = [] # stores the index representations for all words in the headlines\n",
    "loss_mask = [] # 1 for valid words, 0 for padding\n",
    "decoder_x = [] # starts with 1:<SENT>, followed by y[0:len(headline)-1]\n",
    "y_indices = [] # index for the correct decoder prediction, in the\n",
    "skipped_examples = []\n",
    "\n",
    "for hl_i,headline in enumerate(headlines): # processing the output\n",
    "    y_rep = [word_to_index[word] for word in headline]\n",
    "    loss_mask_rep = [1 for i in range(len(y_rep))]\n",
    "    decoder_x_rep = [1]+y_rep[0:len(y_rep)-1]\n",
    "    \n",
    "    non_vocab_dict = x_indices_dicts[hl_i]\n",
    "    y_indices_rep = []\n",
    "    skip_example = False\n",
    "    for word in headline:\n",
    "        if word in fixed_vocab_word_to_index: # word is in fixed_vocabulary\n",
    "            y_indices_rep.append(fixed_vocab_word_to_index[word])\n",
    "        elif word in non_vocab_dict: # word is OOV, use the index assigned to this word in x_indices\n",
    "            y_indices_rep.append(non_vocab_dict[word])\n",
    "        else: # given the small amount of training data, some words in headline have never been seen, removing those examples\n",
    "            skip_example = True\n",
    "            skipped_examples.append(hl_i)\n",
    "            break\n",
    "    if skip_example:\n",
    "        continue\n",
    "    \n",
    "    amount_to_pad = 11-len(y_rep)\n",
    "    y_rep += [0 for i in range(amount_to_pad)]\n",
    "    loss_mask_rep += [0 for i in range(amount_to_pad)] # cancels out loss contribution from padding\n",
    "    decoder_x_rep += [0 for i in range(amount_to_pad)]\n",
    "    y_indices_rep += [0 for i in range(amount_to_pad)] # padding ignored due to loss_mask\n",
    "    y.append(y_rep)\n",
    "    loss_mask.append(loss_mask_rep)\n",
    "    decoder_x.append(decoder_x_rep)\n",
    "    y_indices.append(y_indices_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing scrapped examples\n",
    "x = [x[i] for i in range(len(x)) if i not in skipped_examples]\n",
    "x_indices = [x_indices[i] for i in range(len(x_indices)) if i not in skipped_examples]\n",
    "att_mask = [att_mask[i] for i in range(len(att_mask)) if i not in skipped_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((360, 400), (360, 400), (360, 400), (360, 11), (360, 11), (360, 11))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array(x)\n",
    "x_indices = np.array(x_indices)\n",
    "att_mask = np.array(att_mask)\n",
    "loss_mask = np.array(loss_mask)\n",
    "decoder_x = np.array(decoder_x)\n",
    "y_indices = np.array(y_indices)\n",
    "x.shape,x_indices.shape,att_mask.shape,loss_mask.shape,decoder_x.shape,y_indices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_scatter_nd(updates,indices,batch_size):\n",
    "    \"\"\" applies scatter_nd over the batch dimension\n",
    "    \"\"\"\n",
    "    return tf.convert_to_tensor([tf.scatter_nd(indices[i],updates[i],tf.constant([5347],dtype=tf.int64)) for i in range(batch_size)]) # assuming a max vocab_size+unique_words_in_input of 4947+400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointer_gen_network(embedding_layer,att_w1,att_w2,att_w3,att_v,vocab_d,pgen_w1,pgen_w2,pgen_w3,encoder_h=128,input_len=400,output_len=11,batch_size=30):\n",
    "    \"\"\" Returns pointer generator network\n",
    "    args:\n",
    "        input_len: the length of the input sequence (to the encoder)\n",
    "        output_len: the length of the output sequence (from the decoder)\n",
    "        batch_size: cannot be inferred\n",
    "    \"\"\"\n",
    "    x = Input(shape=(400),dtype=tf.int64) # input to the encoder\n",
    "    x_indices = Input(shape=(400,1),dtype=tf.int64) # represents where each input word prob. should be added in joint prob. vector\n",
    "    att_mask_ = Input(shape=(400)) # mask used with the attention distribution to mask out padding\n",
    "    decoder_x = Input(shape=(11),dtype=tf.int64) # delayed y_data for input to the decoder (for teacher-forcing)\n",
    "    # coverage_vector_ = Input(shape=(400)) ; coverage_vector = coverage_vector_\n",
    "    # s0=Input(shape=(256)) ; s=s0 # initial state for decoder\n",
    "    # c0=Input(shape=(256)) ; c=c0 # initial state for decoder\n",
    "    s = tf.zeros((batch_size,256),dtype=tf.float64) # defining using batch_size makes model brittle, but fine for training\n",
    "    c = tf.zeros((batch_size,256),dtype=tf.float64)\n",
    "    coverage_vector = tf.zeros((batch_size,400),dtype=tf.float64)\n",
    "    att_mask = att_mask_ # must set keras Input() layer to a separate variable before addition\n",
    "    \n",
    "    input_e = embedding_layer(x) # embeddings for the input\n",
    "    h = Bidirectional(LSTM(encoder_h,activation=\"tanh\",return_sequences=True),merge_mode=\"concat\")(input_e) # encoder\n",
    "    \n",
    "    decoder_e = embedding_layer(decoder_x) # embeddings for delayed input to the decoder\n",
    "    outputs = []\n",
    "    coverage_loss_contributions = [] # stores coverage loss contribution for each decoder output step\n",
    "    \n",
    "    for i in range(output_len): # loop through each step of the decoder\n",
    "        decoder_input = decoder_e[:,i,:]  # input to the decoder at this timestep\n",
    "        s,_,c = decoder_lstm(tf.expand_dims(decoder_input,axis=1),initial_state=[s,c])\n",
    "        \n",
    "        # calculating attention (probabilities over input):\n",
    "        s_rep = RepeatVector(input_len)(s) # copying the decoder hidden state\n",
    "        e = att_v(Activation(\"tanh\")(att_w1(h)+att_w2(s_rep)+att_w3(tf.expand_dims(coverage_vector,axis=-1)))) # unscaled attention\n",
    "        e = tf.squeeze(e,axis=-1)+att_mask_ # using attention mask (masks out padding in the input sequence)\n",
    "        a = Activation(\"softmax\")(e) # scaled attention (represents prob. over input)\n",
    "        \n",
    "        # handling coverage vector computations:\n",
    "        step_coverage_loss = tf.reduce_sum(tf.minimum(coverage_vector,a),axis=-1) # cov loss at this decoder step\n",
    "        coverage_loss_contributions.append(step_coverage_loss)\n",
    "        coverage_vector+=a\n",
    "        \n",
    "        # calculating probabilities over fixed vocabulary:\n",
    "        context = Dot(axes=1)([a,h]) # calculating the context vector\n",
    "        pre_vocab_prob = Concatenate()([s,context])\n",
    "        pre_vocab_prob = vocab_d(pre_vocab_prob)\n",
    "        vocab_prob = Activation(\"softmax\")(pre_vocab_prob)\n",
    "        \n",
    "        # calculation probabilty for text generation:\n",
    "        pre_gen_prob = pgen_w1(context)+pgen_w2(s)+pgen_w3(decoder_input)\n",
    "        gen_prob = Activation(\"sigmoid\")(pre_gen_prob)\n",
    "    \n",
    "        # calculating joint-probability for generation/copying:\n",
    "        vocab_prob *= gen_prob # probability of generating a word from the fixed vocabulary\n",
    "        copy_prob = a*(1-gen_prob) # probability of copying a word from the input\n",
    "        \n",
    "        fixed_vocab_indices = tf.tile(tf.reshape(tf.range(4947,dtype=tf.int64),(1,4947,1)),tf.constant([batch_size,1,1])) # 4947 is fixed_vocab size\n",
    "        vocab_prob_projected = apply_scatter_nd(vocab_prob,fixed_vocab_indices,batch_size)\n",
    "        copy_prob_projected = apply_scatter_nd(copy_prob,x_indices,batch_size)\n",
    "        joint_prob = vocab_prob_projected+copy_prob_projected\n",
    "        \n",
    "        outputs.append(joint_prob)\n",
    "    \n",
    "    outputs = K.permute_dimensions(tf.convert_to_tensor(outputs),(1,0,2))\n",
    "    coverage_loss_contributions = K.permute_dimensions(tf.convert_to_tensor(coverage_loss_contributions),(1,0))\n",
    "    \n",
    "    model = Model(inputs=[x,x_indices,decoder_x,att_mask_],outputs=[outputs,coverage_loss_contributions])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(joint_probabilities,y_indices,loss_mask,coverage_loss,lam=1,use_coverage_loss=True):\n",
    "    \"\"\" Returns the loss for this batch\n",
    "    args:\n",
    "        joint_probabilities: joint probability vector for words in input and fixed_vocabulary\n",
    "        y_indices: indices of the correct word in the joint_probabilities vector\n",
    "        loss_mask: vector of 1s,0s specifying whether an input should contribute to the loss\n",
    "        coverage_loss: coverage loss for this batch of examples\n",
    "        lam: hyperparameter determining the contribution of coverage_loss to overall loss\n",
    "        use_coverage_loss: whether coverage loss should be used\n",
    "    \"\"\"\n",
    "    # getting the probabilities for the correct words in joint_probabilities vector (based on y_indices):\n",
    "    y_indices = tf.expand_dims(y_indices,axis=-1)\n",
    "    p_words = tf.squeeze(tf.gather(joint_probabilities,y_indices,batch_dims=2,axis=-1),axis=-1)\n",
    "    p_words = -tf.log(p_words)\n",
    "    p_words *= loss_mask # applying the loss mask\n",
    "    p_words = tf.reduce_sum(p_words,axis=-1)\n",
    "    general_loss_component = tf.reduce_mean(p_words)\n",
    "    \n",
    "    # incorporating the coverage loss:\n",
    "    coverage_loss_component = 0\n",
    "    if use_coverage_loss:\n",
    "        coverage_loss *= loss_mask # applying the loss mask\n",
    "        coverage_loss_component = lam*tf.reduce_mean(coverage_loss)\n",
    "        \n",
    "    total_loss = general_loss_component+coverage_loss_component\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(input_dim=11730,output_dim=100,mask_zero=False) # re-used for both the encoder and decoder\n",
    "decoder_h=256\n",
    "decoder_lstm = LSTM(decoder_h,activation=\"tanh\",return_state=True)\n",
    "att_w1 = Dense(256,use_bias=True,activation=None)\n",
    "att_w2 = Dense(256,use_bias=True,activation=None)\n",
    "att_w3 = Dense(256,use_bias=True,activation=None) # should be 256x1 weight matrix\n",
    "att_v = Dense(1,use_bias=False,activation=None)\n",
    "vocab_d = Dense(4947,use_bias=True,activation=None) # 4947 is fixed_vocabulary size\n",
    "pgen_w1 = Dense(1,use_bias=True,activation=None)\n",
    "pgen_w2 = Dense(1,use_bias=True,activation=None)\n",
    "pgen_w3 = Dense(1,use_bias=True,activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=30\n",
    "model = pointer_gen_network(embedding_layer,att_w1,att_w2,att_w3,att_v,vocab_d,pgen_w1,pgen_w2,pgen_w3,encoder_h=128,input_len=400,output_len=11,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "38.37738777875076\n",
      "28.98354829308011\n",
      "23.394890503689865\n",
      "20.07292970810553\n",
      "17.970521162976926\n",
      "16.280845961475816\n",
      "14.250792061975385\n",
      "13.116590051963628\n",
      "10.929838062667832\n",
      "8.937868055785328\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "for _ in range(10): # epochs\n",
    "    losses = []\n",
    "    for i in range(0,len(x)-batch_size,batch_size):\n",
    "        x_subset = x[i:i+batch_size]\n",
    "        x_indices_subset = x_indices[i:i+batch_size]\n",
    "        x_indices_subset = np.expand_dims(x_indices_subset,axis=-1)\n",
    "        decoder_x_subset = decoder_x[i:i+batch_size]\n",
    "        att_mask_subset = att_mask[i:i+batch_size]\n",
    "        y_indices_subset = y_indices[i:i+batch_size]\n",
    "        loss_mask_subset = loss_mask[i:i+batch_size]\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            joint_probabilities,coverage_loss = model([x_subset,x_indices_subset,decoder_x_subset,att_mask_subset])\n",
    "            loss = loss_function(joint_probabilities,y_indices_subset,loss_mask_subset,coverage_loss,lam=0.1,use_coverage_loss=True)\n",
    "        losses.append(float(loss))\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    print(sum(losses)/max(len(losses),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
