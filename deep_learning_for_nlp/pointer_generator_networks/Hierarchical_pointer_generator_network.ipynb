{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Pointer-Generator Network for Title Generation\n",
    "\n",
    "There are many variations of the Pointer-Generator network; this implementation was based on the following paper: Get To The Point: <i>Summarization with Pointer-Generator Networks</i>, with the difference being a hierarchical component over the sentence-level embeddings (chosen to be the last vector of a sentence as encoded by the bi-directional RNN). this addition is inspired by the following papers: <i>A Hierarchical Neural Autoencoder for Paragraphs and Documents</i> and <i>Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense,LSTM,Input,RepeatVector,Activation,Softmax,Embedding,Dot\n",
    "from tensorflow.keras.layers import Softmax,Concatenate,LayerNormalization,Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__ # required due to a bug with tf.gather which previously did not accumulate gradient flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_cutoff=400 # this is the amount to pad up to for the input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting all of the data\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "data_dir = \"../../data/bbc_news_summary/news_articles/business/\"\n",
    "files = os.listdir(data_dir)\n",
    "num_sents = [] # collects the number of sentences present in each occurance, max number of sentences:21\n",
    "sentence_assignments = [] # assignes a unique number to the sentence assignment for each word\n",
    "\n",
    "headlines = [] # max length for headlines is 11 + 1 (for the <s>)\n",
    "body_texts = [] # max length for body text is 400 (imposed)\n",
    "all_texts = []\n",
    "for fname in files:\n",
    "    with open(data_dir+fname) as data_file:\n",
    "        lines = data_file.readlines()\n",
    "        lines = [line.strip() for line in lines]\n",
    "        body = []\n",
    "        sentence_assignment = []\n",
    "        headline = lines[0]\n",
    "        headline = [tok.text.lower() for tok in nlp(headline)]\n",
    "        doc = nlp(\" \".join(lines[1:]))\n",
    "        for i,sent in enumerate(doc.sents):\n",
    "            sent_tokens = [tok.text.lower() for tok in nlp(sent.text.strip())]\n",
    "            sent_len = len(sent_tokens)\n",
    "            sentence_assignment += [i+1 for _ in range(sent_len)] # index starts at 1, b/c 0 is padding\n",
    "            body += sent_tokens\n",
    "        \n",
    "        body = body[:token_cutoff] # cutting off the length of the body text\n",
    "        sentence_assignment = sentence_assignment[:token_cutoff]\n",
    "        headlines.append(headline)\n",
    "        body_texts.append(body)\n",
    "        sentence_assignments.append(sentence_assignment)\n",
    "        num_sents.append(max(sentence_assignment))\n",
    "        all_texts += body+headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11728 4945 6783\n",
      "['the', '.', ',', 'to', 'of', 'in']\n",
      "['reinforce', 'sufficiently', 'warming', 'chairmanship', 'jacques', 'thabo']\n"
     ]
    }
   ],
   "source": [
    "# getting words which will be part of the fixed_vocabulary (words which appear >= 3 times)\n",
    "word_freq = Counter(all_texts) # there are 11,727 unique words\n",
    "words_by_freq = (list(word_freq.items()))\n",
    "words_by_freq.sort(key=lambda x: x[1],reverse=True) # smaller indices will correspond with more common words\n",
    "most_freq_words = [word_tup[0] for word_tup in words_by_freq if word_tup[1] >= 3] # 4945 words\n",
    "less_freq_words = [word_tup[0] for word_tup in words_by_freq if word_tup[1] < 3] # 6782 words\n",
    "print(len(word_freq),len(most_freq_words),len(less_freq_words))\n",
    "print(most_freq_words[0:6])\n",
    "print(less_freq_words[0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4948"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assigning indices for all words, and adding <PAD>,<SENT>,<UNK> symbols\n",
    "# <UNK> both for inputing words not in fixed_vocab and predicting words not in fixed_vocab or in input sequence\n",
    "word_to_index = {\"<PAD>\":0,\"<SENT>\":1,\"<UNK>\":2} # for all words (including less frequent words)\n",
    "index_to_word = {0:\"<PAD>\",1:\"<SENT>\",2:\"<UNK>\"}\n",
    "\n",
    "fixed_vocab_word_to_index = {\"<PAD>\":0,\"<SENT>\":1,\"<UNK>\":2} # for words assigned to the fixed_vocabulary\n",
    "fixed_vocab_index_to_word = {0:\"<PAD>\",1:\"<SENT>\",2:\"<UNK>\"}\n",
    "\n",
    "index = 3 # starting index for all words\n",
    "# assigning indices to most common words:\n",
    "for word in most_freq_words: \n",
    "    word_to_index[word]=index\n",
    "    index_to_word[index]=word\n",
    "    fixed_vocab_word_to_index[word]=index\n",
    "    fixed_vocab_index_to_word[index]=word\n",
    "    index += 1\n",
    "    \n",
    "# assigning indices to least common words:\n",
    "for word in less_freq_words:\n",
    "    word_to_index[word]=index\n",
    "    index_to_word[index]=word\n",
    "    index += 1\n",
    "\n",
    "len(fixed_vocab_word_to_index) # there are 4948 words in the fixed_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence_data(sentence_assignment,num_max_sents=21,token_cutoff=400):\n",
    "    \"\"\" returns all the sentence-level data required for a given example\n",
    "    \"\"\"\n",
    "    num_sents = len(set(sentence_assignment))\n",
    "    padding_num = num_max_sents-num_sents # amount of padding required\n",
    "    sent_att_mask = [0 for i in range(num_sents)]\n",
    "    sent_att_mask += [-np.inf for i in range(padding_num)]\n",
    "    \n",
    "    single_sent_indices = [] # indices of end of sentence\n",
    "    single_sent_lengths = [] # lengths of each sentence\n",
    "    \n",
    "    curr_assignment_i = 0\n",
    "    curr_assignment = sentence_assignment[0]\n",
    "    for i,assignment in enumerate(sentence_assignment): # denotes the integer representation of the sentence assignment of curr word\n",
    "        if assignment != curr_assignment:\n",
    "            single_sent_lengths.append(i-curr_assignment_i)\n",
    "            single_sent_indices.append(i-1)\n",
    "            curr_assignment = assignment\n",
    "            curr_assignment_i = i\n",
    "            \n",
    "    single_sent_lengths.append(i-curr_assignment_i+1) # +1 necessary for index ordering\n",
    "    single_sent_indices.append(i-1+1)\n",
    "    \n",
    "    single_sent_indices += [0 for _ in range(padding_num)] # padding\n",
    "    single_sent_lengths += [0 for _ in range(padding_num)]\n",
    "    \n",
    "    num_words = sum(single_sent_lengths)\n",
    "    if num_words < token_cutoff: # ensures that sentence-level attention will be the same size as word-level attention\n",
    "        amount_to_pad = token_cutoff-num_words\n",
    "        single_sent_lengths[-1]=amount_to_pad\n",
    "    assert(sum(single_sent_lengths)==token_cutoff)\n",
    "    \n",
    "    return sent_att_mask,single_sent_indices,single_sent_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the input data representations for the model - input is padded up to a length of token_cutoff\n",
    "x = [] # stores the integer/index representation for all input\n",
    "x_indices = [] # stores the joint probability vector indices for all words in the input \n",
    "x_indices_dicts = [] # stores the dicts for assigning words which are not in the fixed_vocabulary\n",
    "att_mask = [] # stores the attention masks (0 for valid words, -np.inf for padding)\n",
    "\n",
    "# new things for the sentence-level hierarchical model:\n",
    "sent_att_masks = [] # mask used for padding the sentence embeddings (makes word-level att. mask obsolete)\n",
    "sent_indices = [] # index of the last vector for each sentence (padded up to length 21)\n",
    "sent_lengths = [] # the length of each of the sentences\n",
    "\n",
    "for bt_i,body_text in enumerate(body_texts): # processing the input\n",
    "    # processing the word-level data:\n",
    "    x_rep = []\n",
    "    for token in body_text:\n",
    "        if token in fixed_vocab_word_to_index:\n",
    "            x_rep.append(fixed_vocab_word_to_index[token])\n",
    "        else:\n",
    "            x_rep.append(fixed_vocab_word_to_index['<UNK>'])\n",
    "    \n",
    "    att_mask_rep = [0 for i in range(len(x_rep))]\n",
    "    amount_to_pad = token_cutoff-len(x_rep)\n",
    "    x_rep += [0 for i in range(amount_to_pad)] # padding the input\n",
    "    att_mask_rep += [-np.inf for i in range(amount_to_pad)]\n",
    "    x.append(x_rep)\n",
    "    att_mask.append(att_mask_rep)\n",
    "    \n",
    "    index = 4948 # starting index for assignment to joint_probability vector\n",
    "    non_vocab_dict = {}\n",
    "    this_x_indices = []\n",
    "    for token in body_text: # assigning each word an index in the joint_probability vector\n",
    "        if token in fixed_vocab_word_to_index:\n",
    "            this_x_indices.append(fixed_vocab_word_to_index[token])\n",
    "        else:\n",
    "            if token in non_vocab_dict: # this word if OOV but has been seen before\n",
    "                this_x_indices.append(non_vocab_dict[token])\n",
    "            else: # this word is OOV and has never been seen before\n",
    "                non_vocab_dict[token]=index\n",
    "                this_x_indices.append(index)\n",
    "                index += 1\n",
    "                \n",
    "    x_indices_dicts.append(non_vocab_dict)\n",
    "    this_x_indices += [0 for i in range(amount_to_pad)] # padding will be masked out in att calculation, so padding with 0 here is valid\n",
    "    x_indices.append(this_x_indices)\n",
    "    \n",
    "    # processing the sentence-level data:\n",
    "    sentence_assignment = sentence_assignments[bt_i]\n",
    "    sent_att_mask,single_sent_indices,single_sent_lengths = process_sentence_data(sentence_assignment)\n",
    "    sent_att_masks.append(sent_att_mask)\n",
    "    sent_indices.append(single_sent_indices)\n",
    "    sent_lengths.append(single_sent_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the largest number of OOV words for a given bid utterances\n",
    "max([len(dic) for dic in x_indices_dicts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the output representations for the model - output is padded up to a length of 11+1 (for final <s> prediction)\n",
    "## all words in output that are not in input utterances or in fixed_vocab_vector are assigned 3:<UNK>\n",
    "y = [] # stores the index representations for all words in the headlines\n",
    "loss_mask = [] # 1 for valid words, 0 for padding\n",
    "decoder_x = [] # starts with 1:<SENT>, followed by y[0:len(headline)-1] (this is the input for teacher-forcing)(12x1)\n",
    "y_indices = [] # index for the correct decoder prediction, in the joint-probability vector\n",
    "\n",
    "for hl_i,headline in enumerate(headlines): # processing the output\n",
    "    \n",
    "    y_rep = [] # not used in the model, stores indices using only fixed_vocab_vector\n",
    "    for token in headline:\n",
    "        if token in fixed_vocab_word_to_index:\n",
    "            y_rep.append(fixed_vocab_word_to_index[token])\n",
    "        else:\n",
    "            y_rep.append(fixed_vocab_word_to_index['<UNK>'])\n",
    "    y_rep.append(fixed_vocab_word_to_index['<SENT>']) # end delimiter of output representation\n",
    "    \n",
    "    loss_mask_rep = [1 for i in range(len(y_rep))]\n",
    "    decoder_x_rep = [1]+y_rep[0:len(y_rep)-1] # embedding word in input but not in fixed_vocab is currently set to <UNK>\n",
    "    amount_to_pad = 12-len(y_rep) # 11+1 represents final <SENT> prediction\n",
    "    y_rep += [0 for i in range(amount_to_pad)]\n",
    "    loss_mask_rep += [0 for i in range(amount_to_pad)] # cancels out loss contribution from padding\n",
    "    decoder_x_rep += [0 for i in range(amount_to_pad)]\n",
    "    \n",
    "    # creating joint-probability representation of output:\n",
    "    non_vocab_dict = x_indices_dicts[hl_i]\n",
    "    y_indices_rep = []\n",
    "    for token in headline:\n",
    "        if token in fixed_vocab_word_to_index: # word is in fixed_vocabulary\n",
    "            y_indices_rep.append(fixed_vocab_word_to_index[token])\n",
    "        elif token in non_vocab_dict: # word is OOV but in the input utterances, use the index assigned to this word in x_indices\n",
    "            y_indices_rep.append(non_vocab_dict[token])\n",
    "        else: # word is OOV and not in input utterances\n",
    "            y_indices_rep.append(fixed_vocab_word_to_index[\"<UNK>\"])\n",
    "    \n",
    "    y_indices_rep.append(fixed_vocab_word_to_index['<SENT>']) # last prediction should be <SENT>\n",
    "    y_indices_rep += [0 for i in range(amount_to_pad)] # padding ignored due to loss_mask\n",
    "    y.append(y_rep)\n",
    "    loss_mask.append(loss_mask_rep)\n",
    "    decoder_x.append(decoder_x_rep)\n",
    "    y_indices.append(y_indices_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(510, 400) (510, 400) (510, 400)\n",
      "(510, 12) (510, 12) (510, 12)\n",
      "(510, 21) (510, 21) (510, 21)\n"
     ]
    }
   ],
   "source": [
    "x = np.array(x)\n",
    "x_indices = np.array(x_indices)\n",
    "att_mask = np.array(att_mask)\n",
    "print(x.shape,x_indices.shape,att_mask.shape)\n",
    "loss_mask = np.array(loss_mask)\n",
    "decoder_x = np.array(decoder_x)\n",
    "y_indices = np.array(y_indices)\n",
    "print(loss_mask.shape,decoder_x.shape,y_indices.shape)\n",
    "sent_att_masks = np.array(sent_att_masks)\n",
    "sent_indices = np.array(sent_indices)\n",
    "sent_lengths = np.array(sent_lengths)\n",
    "print(sent_att_masks.shape,sent_indices.shape,sent_lengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.astype(\"int32\")\n",
    "x_indices = x_indices.astype(\"int32\")\n",
    "att_mask = att_mask.astype(\"float32\")\n",
    "loss_mask = loss_mask.astype(\"int32\")\n",
    "decoder_x = decoder_x.astype(\"int32\")\n",
    "y_indices = y_indices.astype(\"int32\")\n",
    "sent_att_masks = sent_att_masks.astype(\"float32\")\n",
    "sent_indices = sent_indices.astype(\"int32\")\n",
    "sent_lengths = sent_lengths.astype(\"int32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointer-gen architecture code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_scatter_nd(updates,indices,batch_size):\n",
    "    \"\"\" applies scatter_nd over the batch dimension\n",
    "    \"\"\"\n",
    "    return tf.convert_to_tensor([tf.scatter_nd(indices[i],updates[i],tf.constant([5100],dtype=tf.int32)) for i in range(batch_size)]) # assuming a max vocab_size+unique_words_in_input of 4948+102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointer_gen_network(embedding_layer,att_w1,att_w2,att_w3,att_v,sent_att_w1,sent_att_w2,sent_att_v,vocab_d,pgen_w1,pgen_w2,pgen_w3,decoder_lstm,encoder_h=128,input_len=400,output_len=12,max_num_sents=21,batch_size=30):\n",
    "    \"\"\" Returns pointer generator network using Transformer encoder\n",
    "    args:\n",
    "        input_len: the length of the input sequence (to the encoder)\n",
    "        output_len: the length of the output sequence (from the decoder)\n",
    "        batch_size: cannot be inferred so must be explicitly inputted\n",
    "        max_num_sents: the maximum number of unique sentences\n",
    "    \"\"\"\n",
    "    x = Input(shape=(input_len),dtype=tf.int32) # input to the encoder\n",
    "    x_indices_ = Input(shape=(input_len),dtype=tf.int32) # represents where each input word prob. should be added in joint prob. vector\n",
    "    x_indices = tf.expand_dims(x_indices_,axis=-1)\n",
    "    att_mask = Input(shape=(input_len)) # mask used with the attention distribution to mask out padding\n",
    "    decoder_x = Input(shape=(output_len),dtype=tf.int32) # delayed y_data for input to the decoder (for teacher-forcing)\n",
    "    y_indices = Input(shape=(output_len),dtype=tf.int32) # indices of the correct word in the joint_probabilities vector\n",
    "    sent_indices = Input(shape=(max_num_sents),dtype=tf.int32) # sentence-lvl attention components\n",
    "    sent_lengths = Input(shape=(max_num_sents),dtype=tf.int32)\n",
    "    sent_att_mask = Input(shape=(max_num_sents))\n",
    "    s = tf.zeros((batch_size,256),dtype=tf.float32) # defining using batch_size makes model brittle, but fine for training\n",
    "    c = tf.zeros((batch_size,256),dtype=tf.float32)\n",
    "    coverage_vector = tf.zeros((batch_size,input_len),dtype=tf.float32)\n",
    "    \n",
    "    input_e = embedding_layer(x) # embeddings for the input\n",
    "    h = Bidirectional(LSTM(encoder_h,activation=\"tanh\",return_sequences=True),merge_mode=\"concat\")(input_e) # word-level encoder\n",
    "    sentence_h = tf.gather(h,sent_indices,batch_dims=1) # getting sentence-level representations\n",
    "    \n",
    "    decoder_e = embedding_layer(decoder_x) # embeddings for delayed input to the decoder\n",
    "    outputs = []\n",
    "    coverage_loss_contributions = [] # stores coverage loss contribution for each decoder output step\n",
    "    \n",
    "    fixed_vocab_indices = tf.tile(tf.reshape(tf.range(4948,dtype=tf.int32),(1,4948,1)),tf.constant([batch_size,1,1])) # 4948 is fixed_vocab size\n",
    "    \n",
    "    for i in range(output_len): # loop through each step of the decoder\n",
    "        decoder_input = decoder_e[:,i,:]  # input to the decoder at this timestep\n",
    "        s,_,c = decoder_lstm(tf.expand_dims(decoder_input,axis=1),initial_state=[s,c])\n",
    "        \n",
    "        # calculating sentence-level attention:\n",
    "        s_rep = RepeatVector(max_num_sents)(s) # copying the decoder hidden state\n",
    "        sent_e = sent_att_v(Activation(\"tanh\")(sent_att_w1(sentence_h)+sent_att_w2(s_rep)))\n",
    "        sent_e = tf.squeeze(sent_e,axis=-1)+sent_att_mask\n",
    "        sent_a = Activation(\"softmax\")(sent_e) # scaled sentence-level attention\n",
    "        sent_a = tf.convert_to_tensor([tf.repeat(sent_a[i],sent_lengths[i],axis=0) for i in range(batch_size)]) # get to same shape as word-lvl attention\n",
    "        \n",
    "        # calculating word-level attention (probabilities over input tokens):\n",
    "        s_rep = RepeatVector(input_len)(s) # copying the decoder hidden state\n",
    "        e = att_v(Activation(\"tanh\")(att_w1(h)+att_w2(s_rep)+att_w3(tf.expand_dims(coverage_vector,axis=-1)))) # unscaled attention\n",
    "        e = tf.squeeze(e,axis=-1)+att_mask # using attention mask (masks out padding in the input sequence)\n",
    "        a = Activation(\"softmax\")(e) # scaled attention (represents prob. over input)\n",
    "        \n",
    "        # calculating hierarchical attention:\n",
    "        a = a*sent_a # updating word-level attention based on sentence-level attention\n",
    "        a = Activation(\"softmax\")(a)\n",
    "        \n",
    "        # handling coverage vector computations:\n",
    "        step_coverage_loss = tf.reduce_sum(tf.minimum(coverage_vector,a),axis=-1) # cov loss at this decoder step\n",
    "        coverage_loss_contributions.append(step_coverage_loss)\n",
    "        coverage_vector+=a\n",
    "        \n",
    "        # calculating probabilities over fixed vocabulary:\n",
    "        context = Dot(axes=1)([a,h]) # calculating the context vector\n",
    "        pre_vocab_prob = Concatenate()([s,context]) # 512 dimentional\n",
    "        pre_vocab_prob = vocab_d(pre_vocab_prob) # could reduce memory here by first reducing this to < 512 dim. with a linear layer\n",
    "        vocab_prob = Activation(\"softmax\")(pre_vocab_prob)\n",
    "        \n",
    "        # calculation probabilty for text generation:\n",
    "        pre_gen_prob = pgen_w1(context)+pgen_w2(s)+pgen_w3(decoder_input)\n",
    "        gen_prob = Activation(\"sigmoid\")(pre_gen_prob)\n",
    "    \n",
    "        # calculating joint-probability for generation/copying:\n",
    "        vocab_prob *= gen_prob # probability of generating a word from the fixed vocabulary\n",
    "        copy_prob = a*(1-gen_prob) # probability of copying a word from the input\n",
    "        \n",
    "        vocab_prob_projected = apply_scatter_nd(vocab_prob,fixed_vocab_indices,batch_size)\n",
    "        copy_prob_projected = apply_scatter_nd(copy_prob,x_indices,batch_size)\n",
    "        joint_prob = vocab_prob_projected+copy_prob_projected\n",
    "        \n",
    "        # gathering predictions from joint-probability vector - doing it here will reduce memory consumption\n",
    "        y_indices_i = tf.expand_dims(y_indices[:,i],axis=-1) # getting predictions at time i for whole batch\n",
    "        predictions_i = tf.squeeze(tf.gather(joint_prob,y_indices_i,batch_dims=1,axis=-1),axis=-1)\n",
    "        outputs.append(predictions_i)\n",
    "    \n",
    "    outputs = K.permute_dimensions(tf.convert_to_tensor(outputs),(1,0))\n",
    "    coverage_loss_contributions = K.permute_dimensions(tf.convert_to_tensor(coverage_loss_contributions),(1,0))\n",
    "    \n",
    "    model = Model(inputs=[x,x_indices_,decoder_x,att_mask,y_indices,sent_indices,sent_lengths,sent_att_mask],outputs=[outputs,coverage_loss_contributions])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(prediction_probabilities,loss_mask,coverage_loss,lam=0.1,use_coverage_loss=True):\n",
    "    \"\"\" Returns the loss for this batch\n",
    "    args:\n",
    "        prediction_probabilities: model-assigned probabilities for ground-truth predictions\n",
    "        loss_mask: vector of 1s,0s specifying whether an input should contribute to the loss\n",
    "        coverage_loss: coverage loss for this batch of examples\n",
    "        lam: hyperparameter determining the contribution of coverage_loss to overall loss\n",
    "        use_coverage_loss: whether coverage loss should be used\n",
    "    \"\"\"\n",
    "    p_words = -tf.math.log(prediction_probabilities)\n",
    "    p_words *= loss_mask # applying the loss mask\n",
    "    p_words = tf.reduce_sum(p_words,axis=-1)\n",
    "    general_loss_component = tf.reduce_mean(p_words)\n",
    "    \n",
    "    # incorporating the coverage loss:\n",
    "    coverage_loss_component = 0\n",
    "    if use_coverage_loss:\n",
    "        coverage_loss *= loss_mask # applying the loss mask\n",
    "        coverage_loss = tf.reduce_sum(coverage_loss,axis=-1)\n",
    "        coverage_loss_component = lam*tf.reduce_mean(coverage_loss)\n",
    "        \n",
    "    total_loss = general_loss_component+coverage_loss_component\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(input_dim=4950,output_dim=128,mask_zero=True) # re-used for both the encoder and decoder\n",
    "decoder_h=256\n",
    "decoder_lstm = LSTM(decoder_h,activation=\"tanh\",return_state=True)\n",
    "att_w1 = Dense(256,use_bias=True,activation=None) # for word-level attention\n",
    "att_w2 = Dense(256,use_bias=True,activation=None)\n",
    "att_w3 = Dense(256,use_bias=True,activation=None)\n",
    "att_v = Dense(1,use_bias=False,activation=None)\n",
    "sent_att_w1 = Dense(256,use_bias=True,activation=None) # for sentence-level attention\n",
    "sent_att_w2 = Dense(256,use_bias=True,activation=None)\n",
    "sent_att_v = Dense(1,use_bias=False,activation=None)\n",
    "vocab_d = Dense(4948,use_bias=True,activation=None) # 4948 is fixed_vocabulary size\n",
    "pgen_w1 = Dense(1,use_bias=True,activation=None)\n",
    "pgen_w2 = Dense(1,use_bias=True,activation=None)\n",
    "pgen_w3 = Dense(1,use_bias=True,activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size=10\n",
    "model = pointer_gen_network(embedding_layer,att_w1,att_w2,att_w3,att_v,sent_att_w1,sent_att_w2,sent_att_v,vocab_d,pgen_w1,pgen_w2,pgen_w3,decoder_lstm,encoder_h=128,batch_size=batch_size)\n",
    "optimizer = Adam(lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.100582122802734\n",
      "34.27932548522949\n",
      "31.368357552422417\n",
      "30.25632158915202\n",
      "29.239930894639755\n",
      "28.66851086086697\n",
      "26.980822457207573\n",
      "24.771902508205837\n",
      "22.86968782212999\n",
      "20.986548105875652\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "\n",
    "for _ in range(10): # epochs\n",
    "    losses = []\n",
    "    for i in range(0,100-batch_size,batch_size): # only using first 100 samples for training\n",
    "        x_subset = x[i:i+batch_size]\n",
    "        x_indices_subset = x_indices[i:i+batch_size]\n",
    "        decoder_x_subset = decoder_x[i:i+batch_size]\n",
    "        att_mask_subset = att_mask[i:i+batch_size]\n",
    "        y_indices_subset = y_indices[i:i+batch_size]\n",
    "        loss_mask_subset = loss_mask[i:i+batch_size]\n",
    "        sent_indices_subset = sent_indices[i:i+batch_size]\n",
    "        sent_lengths_subset = sent_lengths[i:i+batch_size]\n",
    "        sent_att_mask_subset = sent_att_masks[i:i+batch_size]\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction_probabilities,coverage_loss = model([x_subset,x_indices_subset,decoder_x_subset,att_mask_subset,y_indices_subset,sent_indices_subset,sent_lengths_subset,sent_att_mask_subset])\n",
    "            loss = loss_function(prediction_probabilities,loss_mask_subset,coverage_loss,lam=0.1,use_coverage_loss=True)\n",
    "        losses.append(float(loss))\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    print(sum(losses)/max(len(losses),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
