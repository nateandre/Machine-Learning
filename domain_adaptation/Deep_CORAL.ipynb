{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of deep CORAL\n",
    "\n",
    "This is based on the paper: <i>Deep CORAL: Correlation Alignment for Deep Domain Adaptation</i>. In this case I am not fine-tuning a pretrained model but rather using a pretrained feature extractor. The task is sentiment analysis in which the target domain (Yelp reviews) has no labeled data (Amazon reviews are the source)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input,Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,balanced_accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__ # required for tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 640) (10000,)\n"
     ]
    }
   ],
   "source": [
    "## Getting the Yelp review data\n",
    "yelp_x = np.load(\"../data/yelp_reviews/x.npy\")\n",
    "yelp_y = np.load(\"../data/yelp_reviews/y.npy\")\n",
    "yelp_x = yelp_x[:10000]\n",
    "yelp_y = yelp_y[:10000]\n",
    "print(yelp_x.shape,yelp_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 640) (10000,) (1000, 640) (1000,)\n"
     ]
    }
   ],
   "source": [
    "## Getting the Amazon review data\n",
    "amazon_x = np.load(\"../data/amazon_reviews/x.npy\")\n",
    "amazon_y = np.load(\"../data/amazon_reviews/y.npy\")\n",
    "amazon_x,amazon_y = amazon_x[:11000],amazon_y[:11000]\n",
    "amazon_x_val,amazon_y_val = amazon_x[10000:],amazon_y[10000:]\n",
    "amazon_x,amazon_y = amazon_x[:10000],amazon_y[:10000]\n",
    "print(amazon_x.shape,amazon_y.shape,amazon_x_val.shape,amazon_y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model implementation\n",
    "\n",
    "$$ L_{CORAL} = \\frac{1}{4d^2}||C_S - C_T||^2_F $$\n",
    "\n",
    "Where d is the model layer dim. and $ C_S \\& C_T $ are the batch feature covariance matrices for the source and target domains respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_dim=640):\n",
    "    \"\"\" model implementation\n",
    "        -also returns the last hidden state (to use w/ CORAL loss component)\n",
    "        -using the h2 vector prior to the relu activation makes little difference in practice\n",
    "    \"\"\"\n",
    "    x = Input(shape=(input_dim))\n",
    "    h1 = Dense(512,activation='relu')(x)\n",
    "    h2 = Dense(256,activation='relu')(h1)\n",
    "    out = Dense(1,activation='sigmoid')(h2)\n",
    "    \n",
    "    model = Model(inputs=x,outputs=[out,h2])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coral_loss(mat1,mat2,d=256):\n",
    "    \"\"\" calculates the CORAL loss component\n",
    "    args:\n",
    "        d: dimensionality of the input model hidden layer\n",
    "    \"\"\"\n",
    "    mat1_cov = K.flatten(tfp.stats.covariance(mat1))\n",
    "    mat2_cov = K.flatten(tfp.stats.covariance(mat2))\n",
    "    squared_frobenius_distance = (1/(4*d**2))*tf.reduce_sum(tf.square(mat1_cov-mat2_cov)) # removed tf.sqrt()\n",
    "    return squared_frobenius_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(amazon_y_subset,amazon_pred,amazon_h,yelp_h,coral_lam):\n",
    "    \"\"\" loss implementation\n",
    "    args:\n",
    "        coral_lam: amount to scale coral loss component by\n",
    "    \"\"\"\n",
    "    class_loss = BinaryCrossentropy()(amazon_y_subset,amazon_pred) # automatic avg over batch\n",
    "    coral_loss = get_coral_loss(amazon_h,yelp_h)\n",
    "    total_loss = class_loss+(coral_lam*coral_loss)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_model(model,optimizer,amazon_x_subset,amazon_y_subset,yelp_x_subset,coral_lam=1.0):\n",
    "    \"\"\" used to train the model\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        amazon_pred,amazon_h = model(amazon_x_subset)\n",
    "        _,yelp_h = model(yelp_x_subset)\n",
    "        loss = model_loss(amazon_y_subset,amazon_pred,amazon_h,yelp_h,coral_lam)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; loss:0.3801\n",
      "-Train; accuracy:0.8874; bal_accuracy:0.8878\n",
      "-Test; accuracy:0.87; bal_accuracy:0.8693\n",
      "-YELP; accuracy:0.8738; bal_accuracy:0.8758\n",
      "epoch 1; loss:0.3005\n",
      "-Train; accuracy:0.8926; bal_accuracy:0.8927\n",
      "-Test; accuracy:0.878; bal_accuracy:0.8776\n",
      "-YELP; accuracy:0.8661; bal_accuracy:0.8692\n",
      "epoch 2; loss:0.2718\n",
      "-Train; accuracy:0.9012; bal_accuracy:0.9013\n",
      "-Test; accuracy:0.878; bal_accuracy:0.8777\n",
      "-YELP; accuracy:0.8592; bal_accuracy:0.8625\n"
     ]
    }
   ],
   "source": [
    "batch_size=50\n",
    "optimizer = Adam(lr=0.01)\n",
    "epochs=3\n",
    "\n",
    "for epoch_i in range(epochs):\n",
    "    losses = []\n",
    "    for i in range(0,len(amazon_x),batch_size):\n",
    "        x_train_subset = amazon_x[i:i+batch_size]\n",
    "        y_train_subset = amazon_y[i:i+batch_size]\n",
    "        yelp_x_subset = yelp_x[i:i+batch_size]\n",
    "        batch_loss = train_model(model,optimizer,x_train_subset,y_train_subset,yelp_x_subset)\n",
    "        losses.append(float(batch_loss))\n",
    "    \n",
    "    print(\"epoch {}; loss:{}\".format(epoch_i,round(sum(losses)/len(losses),4)))\n",
    "    y_train_pred,_ = model(amazon_x)\n",
    "    y_train_pred = y_train_pred.numpy()\n",
    "    y_train_pred[y_train_pred >= 0.5]=1 ; y_train_pred[y_train_pred < 0.5]=0\n",
    "    \n",
    "    y_val_pred,_ = model(amazon_x_val)\n",
    "    y_val_pred = y_val_pred.numpy()\n",
    "    y_val_pred[y_val_pred >= 0.5]=1 ; y_val_pred[y_val_pred < 0.5]=0\n",
    "    \n",
    "    yelp_pred,_ = model(yelp_x)\n",
    "    yelp_pred = yelp_pred.numpy()\n",
    "    yelp_pred[yelp_pred >= 0.5]=1 ; yelp_pred[yelp_pred < 0.5]=0\n",
    "    \n",
    "    train_acc,train_bal_acc = round(accuracy_score(amazon_y,y_train_pred),4),round(balanced_accuracy_score(amazon_y,y_train_pred),4)    \n",
    "    val_acc,val_bal_acc = round(accuracy_score(amazon_y_val,y_val_pred),4),round(balanced_accuracy_score(amazon_y_val,y_val_pred),4)\n",
    "    yelp_acc,yelp_bal_acc = round(accuracy_score(yelp_y,yelp_pred),4),round(balanced_accuracy_score(yelp_y,yelp_pred),4)\n",
    "    \n",
    "    print(\"-Train; accuracy:{}; bal_accuracy:{}\".format(train_acc,train_bal_acc))\n",
    "    print(\"-Test; accuracy:{}; bal_accuracy:{}\".format(val_acc,val_bal_acc))\n",
    "    print(\"-YELP; accuracy:{}; bal_accuracy:{}\".format(yelp_acc,yelp_bal_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
