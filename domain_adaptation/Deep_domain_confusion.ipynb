{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of deep domain confusion\n",
    "\n",
    "This is based on the paper: <i>Deep Domain Confusion: Maximizing for Domain Invariance</i>. In this case I am not fine-tuning a pretrained model but rather using a pretrained feature extractor. The task is sentiment analysis in which the target domain (Yelp reviews) has no labeled data (Amazon reviews are the source)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input,Dense,Dropout,Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,balanced_accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## feature extraction using pretrained feature extractor\n",
    "universal_embed = hub.load(\"../other/universal-sentence-encoder_4\")\n",
    "nnlm_embed = hub.load(\"../other/nnlm-en-dim128_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 640) (10000,)\n"
     ]
    }
   ],
   "source": [
    "## Getting the Yelp review data\n",
    "\"\"\"\n",
    "all_yelp_reviews = pd.read_csv('../data/yelp_reviews/train.csv')\n",
    "all_feats = []\n",
    "all_labels = []\n",
    "for _,row in all_yelp_reviews.iterrows():\n",
    "    label,string = int(row['score'])-1,row['review'].strip().lower()\n",
    "    string = np.expand_dims(np.asarray(string),axis=0)\n",
    "    all_feats.append(np.hstack([universal_embed(string),nnlm_embed(string)]))\n",
    "    all_labels.append(label)\n",
    "\n",
    "x = np.vstack(all_feats)\n",
    "y = np.asarray(all_labels).astype(\"int32\")\n",
    "\"\"\"\n",
    "yelp_x = np.load(\"../data/yelp_reviews/x.npy\")\n",
    "yelp_y = np.load(\"../data/yelp_reviews/y.npy\")\n",
    "yelp_x = yelp_x[:10000]\n",
    "yelp_y = yelp_y[:10000]\n",
    "print(yelp_x.shape,yelp_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 640) (10000,) (1000, 640) (1000,)\n"
     ]
    }
   ],
   "source": [
    "## Getting the Amazon review data\n",
    "amazon_x = np.load(\"../data/amazon_reviews/x.npy\")\n",
    "amazon_y = np.load(\"../data/amazon_reviews/y.npy\")\n",
    "amazon_x,amazon_y = amazon_x[:11000],amazon_y[:11000]\n",
    "amazon_x_val,amazon_y_val = amazon_x[10000:],amazon_y[10000:]\n",
    "amazon_x,amazon_y = amazon_x[:10000],amazon_y[:10000]\n",
    "print(amazon_x.shape,amazon_y.shape,amazon_x_val.shape,amazon_y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model implementation\n",
    "\n",
    "As defined in <i>Beyond Sharing Weights for Deep Domain Adaptation<i>:\n",
    "$$ MMD^2({f^s_i},{f^t_j}) = \\sum_{i,i^`}\\frac{k(f^s_i,f^s_{i^`})}{(N^s)^2} + \\sum_{j,j^`}\\frac{k(f^t_j,f^t_{j^`})}{(N^t)^2} - 2 \\sum_{i,j}\\frac{k(f^s_i,f^t_j)}{N^s N^t} $$\n",
    "\n",
    "$$ k(u,v) = exp(-|| u-v ||^2 / \\sigma) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_dim=640):\n",
    "    \"\"\" model implementation\n",
    "        -also returns the last hidden state (to use w/ MMD loss component)\n",
    "        -using the h2 vector prior to the relu activation makes no difference in practice\n",
    "    \"\"\"\n",
    "    x = Input(shape=(input_dim))\n",
    "    h1 = Dense(512,activation='relu')(x)\n",
    "    h2 = Dense(256,activation='relu')(h1)\n",
    "    out = Dense(1,activation='sigmoid')(h2)\n",
    "    \n",
    "    model = Model(inputs=x,outputs=[out,h2])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mmd_unit(mat1,mat2,sigma=1,c=1e-18):\n",
    "    \"\"\" calculates MMD components\n",
    "        -c is used to stabilize gradient flow\n",
    "    \"\"\"\n",
    "    mat1 = tf.expand_dims(mat1,axis=0) # done for tf broadcasting\n",
    "    mat2 = tf.expand_dims(mat2,axis=1)\n",
    "    diff = tf.reshape(tf.subtract(mat1,mat2),[-1,2]) # difference between all rows in mat1 and mat2, stacked\n",
    "    squared_euclid_distance = tf.sqrt(tf.reduce_sum(tf.square(diff),axis=1)+c)\n",
    "    kernel_sum = tf.reduce_mean(tf.exp(-squared_euclid_distance/sigma)) # calculating RBF kernel\n",
    "    return kernel_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddc_loss(amazon_y_subset,amazon_pred,amazon_h,yelp_h,mmd_lam):\n",
    "    \"\"\" DDC loss implementation\n",
    "    args:\n",
    "        mmd_lam: amount to scale MMD loss component by\n",
    "    \"\"\"\n",
    "    class_loss = BinaryCrossentropy()(amazon_y_subset,amazon_pred) # automatic avg over batch\n",
    "    mmd_loss = get_mmd_unit(amazon_h,amazon_h)+get_mmd_unit(yelp_h,yelp_h)-2*get_mmd_unit(yelp_h,amazon_h)\n",
    "    total_loss = class_loss+(mmd_lam*mmd_loss)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_model(model,optimizer,amazon_x_subset,amazon_y_subset,yelp_x_subset,mmd_lam=0.25):\n",
    "    \"\"\" used to train the model\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        amazon_pred,amazon_h = model(amazon_x_subset)\n",
    "        _,yelp_h = model(yelp_x_subset)\n",
    "        loss = ddc_loss(amazon_y_subset,amazon_pred,amazon_h,yelp_h,mmd_lam)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; loss:0.3808\n",
      "-Train; accuracy:0.8855; bal_accuracy:0.8859\n",
      "-Test; accuracy:0.874; bal_accuracy:0.8734\n",
      "-YELP; accuracy:0.8698; bal_accuracy:0.8724\n",
      "epoch 1; loss:0.3009\n",
      "-Train; accuracy:0.8956; bal_accuracy:0.8958\n",
      "-Test; accuracy:0.877; bal_accuracy:0.8763\n",
      "-YELP; accuracy:0.8625; bal_accuracy:0.8661\n",
      "epoch 2; loss:0.2752\n",
      "-Train; accuracy:0.8995; bal_accuracy:0.9003\n",
      "-Test; accuracy:0.88; bal_accuracy:0.8788\n",
      "-YELP; accuracy:0.8708; bal_accuracy:0.8713\n"
     ]
    }
   ],
   "source": [
    "# In practice the MMD loss seems to helps stabilize the learning so that the Yelp performance degrades less during training\n",
    "batch_size=50\n",
    "optimizer = Adam(lr=0.01)\n",
    "epochs=3\n",
    "\n",
    "for epoch_i in range(epochs):\n",
    "    losses = []\n",
    "    for i in range(0,len(amazon_x),batch_size):\n",
    "        x_train_subset = amazon_x[i:i+batch_size]\n",
    "        y_train_subset = amazon_y[i:i+batch_size]\n",
    "        yelp_x_subset = yelp_x[i:i+batch_size]\n",
    "        batch_loss = train_model(model,optimizer,x_train_subset,y_train_subset,yelp_x_subset)\n",
    "        losses.append(float(batch_loss))\n",
    "    \n",
    "    print(\"epoch {}; loss:{}\".format(epoch_i,round(sum(losses)/len(losses),4)))\n",
    "    y_train_pred,_ = model(amazon_x)\n",
    "    y_train_pred = y_train_pred.numpy()\n",
    "    y_train_pred[y_train_pred >= 0.5]=1 ; y_train_pred[y_train_pred < 0.5]=0\n",
    "    \n",
    "    y_val_pred,_ = model(amazon_x_val)\n",
    "    y_val_pred = y_val_pred.numpy()\n",
    "    y_val_pred[y_val_pred >= 0.5]=1 ; y_val_pred[y_val_pred < 0.5]=0\n",
    "    \n",
    "    yelp_pred,_ = model(yelp_x)\n",
    "    yelp_pred = yelp_pred.numpy()\n",
    "    yelp_pred[yelp_pred >= 0.5]=1 ; yelp_pred[yelp_pred < 0.5]=0\n",
    "    \n",
    "    train_acc,train_bal_acc = round(accuracy_score(amazon_y,y_train_pred),4),round(balanced_accuracy_score(amazon_y,y_train_pred),4)    \n",
    "    val_acc,val_bal_acc = round(accuracy_score(amazon_y_val,y_val_pred),4),round(balanced_accuracy_score(amazon_y_val,y_val_pred),4)\n",
    "    yelp_acc,yelp_bal_acc = round(accuracy_score(yelp_y,yelp_pred),4),round(balanced_accuracy_score(yelp_y,yelp_pred),4)\n",
    "    \n",
    "    print(\"-Train; accuracy:{}; bal_accuracy:{}\".format(train_acc,train_bal_acc))\n",
    "    print(\"-Test; accuracy:{}; bal_accuracy:{}\".format(val_acc,val_bal_acc))\n",
    "    print(\"-YELP; accuracy:{}; bal_accuracy:{}\".format(yelp_acc,yelp_bal_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_standard_model(input_dim=640):\n",
    "    x = Input(shape=(input_dim))\n",
    "    h = Dense(512,activation='relu')(x)\n",
    "    h = Dense(256,activation='relu')(h)\n",
    "    out = Dense(1,activation='sigmoid')(h)\n",
    "    model = Model(inputs=x,outputs=out)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=Adam(lr=0.01)) # train_on_batch()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_standard_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=50\n",
    "epochs=10\n",
    "\n",
    "for epoch_i in range(epochs):\n",
    "    losses = []\n",
    "    for i in range(0,len(amazon_x),batch_size):\n",
    "        x_train_subset = amazon_x[i:i+batch_size]\n",
    "        y_train_subset = amazon_y[i:i+batch_size]\n",
    "        batch_loss = model.train_on_batch(x_train_subset,y_train_subset)\n",
    "        losses.append(float(batch_loss))\n",
    "    \n",
    "    print(\"epoch {}; loss:{}\".format(epoch_i,round(sum(losses)/len(losses),4)))\n",
    "    y_train_pred = model(amazon_x).numpy()\n",
    "    y_train_pred[y_train_pred >= 0.5]=1 ; y_train_pred[y_train_pred < 0.5]=0\n",
    "    \n",
    "    y_val_pred = model(amazon_x_val).numpy()\n",
    "    y_val_pred[y_val_pred >= 0.5]=1 ; y_val_pred[y_val_pred < 0.5]=0\n",
    "    \n",
    "    yelp_pred = model(yelp_x).numpy()\n",
    "    yelp_pred[yelp_pred >= 0.5]=1 ; yelp_pred[yelp_pred < 0.5]=0\n",
    "    \n",
    "    train_acc,train_bal_acc = round(accuracy_score(amazon_y,y_train_pred),4),round(balanced_accuracy_score(amazon_y,y_train_pred),4)    \n",
    "    val_acc,val_bal_acc = round(accuracy_score(amazon_y_val,y_val_pred),4),round(balanced_accuracy_score(amazon_y_val,y_val_pred),4)\n",
    "    yelp_acc,yelp_bal_acc = round(accuracy_score(yelp_y,yelp_pred),4),round(balanced_accuracy_score(yelp_y,yelp_pred),4)\n",
    "    \n",
    "    print(\"-Train; accuracy:{}; bal_accuracy:{}\".format(train_acc,train_bal_acc))\n",
    "    print(\"-Test; accuracy:{}; bal_accuracy:{}\".format(val_acc,val_bal_acc))\n",
    "    print(\"-YELP; accuracy:{}; bal_accuracy:{}\".format(yelp_acc,yelp_bal_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
